{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TorQ in Managed kdb Insights Demo Pack The purpose of the TorQ on Managed kdb Insights Demo Pack is to set up an example TorQ installation on Managed kdb Insights and to show how applications can be built and deployed on top of the TorQ framework. The example installation contains key features of a production data capture installation, including persistence. The demo pack includes: an example set of historic data a simulated data feed configuration changes for base TorQ start and stop scripts using Terraform Once started, TorQ will generate simulated data and push it into an in-memory real-time database inside the rdb cluster. It will persist this data to a Managed kdb database every day at midnight. The system will operate 24*7. email: torqsupport@dataintellect.com web: www.dataintellect.com","title":"Home"},{"location":"#torq-in-managed-kdb-insights-demo-pack","text":"The purpose of the TorQ on Managed kdb Insights Demo Pack is to set up an example TorQ installation on Managed kdb Insights and to show how applications can be built and deployed on top of the TorQ framework. The example installation contains key features of a production data capture installation, including persistence. The demo pack includes: an example set of historic data a simulated data feed configuration changes for base TorQ start and stop scripts using Terraform Once started, TorQ will generate simulated data and push it into an in-memory real-time database inside the rdb cluster. It will persist this data to a Managed kdb database every day at midnight. The system will operate 24*7. email: torqsupport@dataintellect.com web: www.dataintellect.com","title":"TorQ in Managed kdb Insights Demo Pack"},{"location":"01-whatistorq/","text":"What Is TorQ? Data Intellect TorQ is a comprehensive framework built on top of the kdb+ database system. It serves as the foundation for creating production-ready kdb+ systems by providing core functionality and utilities that allow developers to focus on building application business logic. TorQ incorporates numerous best practices, emphasizing performance, maintainability, supportability and extensibility. Key features of Data Intellect TorQ include process management, code management, configuration management, usage logging, connection management (both incoming and outgoing), access controls, timer extensions, standard output/error logging, error handling, documentation and development tools. Currently TorQ within Managed kdb Insights is a minimum viable product (MVP), meaning that not all TorQ functionality has been implemented but in future iterations more functionality will be added. If you would like to read more information on TorQ and all of its functionality, please follow this link . Additional background on this project is here .","title":"Workshop 01 - What is TorQ?"},{"location":"01-whatistorq/#what-is-torq","text":"Data Intellect TorQ is a comprehensive framework built on top of the kdb+ database system. It serves as the foundation for creating production-ready kdb+ systems by providing core functionality and utilities that allow developers to focus on building application business logic. TorQ incorporates numerous best practices, emphasizing performance, maintainability, supportability and extensibility. Key features of Data Intellect TorQ include process management, code management, configuration management, usage logging, connection management (both incoming and outgoing), access controls, timer extensions, standard output/error logging, error handling, documentation and development tools. Currently TorQ within Managed kdb Insights is a minimum viable product (MVP), meaning that not all TorQ functionality has been implemented but in future iterations more functionality will be added. If you would like to read more information on TorQ and all of its functionality, please follow this link . Additional background on this project is here .","title":"What Is TorQ?"},{"location":"02-Introduction/","text":"Introduction Goals of the workshop To understand what TorQ is and to know the benefits of using it with Managed kdb Insights. To set up a TorQ stack within Managed kdb Insights, connect to the clusters, and access the data within - both historical and real-time. Show existing TorQ users how to migrate their database into Managed kdb Insights. What are we going to build? In this workshop we are going to be continuing on from the previous kdb environment setup , and building \u201cTorQ for Amazon FinSpace with Managed kdb Insights\u201d, a MVP of TorQ which is leveraging functionality within AWS. In this MVP, although all the TorQ code will be included within your code bucket, we will only be using files which are a necessity for this MVP creation. This will create a working TorQ setup on the cloud through Managed kdb Insights. We are going to do so by replicating the below steps. Create a General Purpose (GP) cluster for the Discovery process of TorQ. This allows other processes to use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability. Create an RDB cluster. This will allow us to query and store live data from the feed. Create a HDB cluster. This will allow us to query historical data. Create Gateway cluster which acts as the gateway within TorQ. This process allows users to query data within the RDB and HDB processes. Lastly, create another General Purpose (GP) cluster within Managed kdb Insights. This will replicate the feed handler of TorQ, which will normalize and prepare our data into a schema readable by kdb, for the ingestion and population of our tables. All of this culminates in a TorQ production system being hosted on the cloud using five general purpose clusters. This allows users to ingest data, before querying both live and historical data through a gateway and discovery process.","title":"Workshop 02 - Introduction"},{"location":"02-Introduction/#introduction","text":"","title":"Introduction"},{"location":"02-Introduction/#goals-of-the-workshop","text":"To understand what TorQ is and to know the benefits of using it with Managed kdb Insights. To set up a TorQ stack within Managed kdb Insights, connect to the clusters, and access the data within - both historical and real-time. Show existing TorQ users how to migrate their database into Managed kdb Insights.","title":"Goals of the workshop"},{"location":"02-Introduction/#what-are-we-going-to-build","text":"In this workshop we are going to be continuing on from the previous kdb environment setup , and building \u201cTorQ for Amazon FinSpace with Managed kdb Insights\u201d, a MVP of TorQ which is leveraging functionality within AWS. In this MVP, although all the TorQ code will be included within your code bucket, we will only be using files which are a necessity for this MVP creation. This will create a working TorQ setup on the cloud through Managed kdb Insights. We are going to do so by replicating the below steps. Create a General Purpose (GP) cluster for the Discovery process of TorQ. This allows other processes to use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability. Create an RDB cluster. This will allow us to query and store live data from the feed. Create a HDB cluster. This will allow us to query historical data. Create Gateway cluster which acts as the gateway within TorQ. This process allows users to query data within the RDB and HDB processes. Lastly, create another General Purpose (GP) cluster within Managed kdb Insights. This will replicate the feed handler of TorQ, which will normalize and prepare our data into a schema readable by kdb, for the ingestion and population of our tables. All of this culminates in a TorQ production system being hosted on the cloud using five general purpose clusters. This allows users to ingest data, before querying both live and historical data through a gateway and discovery process.","title":"What are we going to build?"},{"location":"03-torqwithmanagedkdbinsights/","text":"TorQ with Managed kdb Insights When porting TorQ to AWS we chose to make a minimum viable product and then build out features on top of this to give clients a working AWS solution that includes the essential TorQ features. We intend to align with the Managed kdb Insights roadmap, adding additional functionality to the TorQ implementation as new features become available. The processes available in our first iteration of TorQ for Amazon FinSpace with Managed kdb Insights are the following: Discovery: Processes use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections - it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections. Historical Database: The HDB holds data from before the current period. It is read only and used for querying all historical data. Data is stored in int partitions (one per period) and can be queried through the gateway process. Real-time Database: The RDB subscribes and captures all data from the feed handler throughout the current period and store it in memory for query or real-time processing. Feed Handler: The feed handler acts as a preparation stage for the data, transforming the data into kdb+ format and writing it to our RDB. Gateway: The gateway acts as a single interface point that separates the end user from the configuration of underlying databases. You don't need to know where data is stored, and you don't need to make multiple requests to retrieve it. It can access a single process, or join data across multiple processes. It also does load balancing and implements a level of resilience by hiding back-end process failure from clients. These features allow us to store real-time and historical data and make it available to users. Notable Differences within this reduced version of TorQ in comparison to normal TorQ env.q The entry point script is env.q which sets initial environment variables and loads the main torq.q script. This is used because Managed kdb Insights does not allow setting environment variables or running shell scripts at startup. The \u201cenv.q\u201d file is the first of our files loaded in and it then specifies based on start-up parameters what to load in next. It also references and connects to the setup created inside Managed kdb Insights . For example, the database name for your AWS environment is referenced inside of this file. Loading code/hdbs Generally the main process code file (e.g. code/processes/discovery.q ) is passed as a command line parameter with the -load flag. We can't do this in Managed kdb Insights as filepaths are not allowed as command line parameters (yet). To work around this, we include a .proc.params[`load] variable in the settings file. We use the same approach for loading data directories into HDBs.","title":"Workshop 03 - TorQ With Managed kdb Insights"},{"location":"03-torqwithmanagedkdbinsights/#torq-with-managed-kdb-insights","text":"When porting TorQ to AWS we chose to make a minimum viable product and then build out features on top of this to give clients a working AWS solution that includes the essential TorQ features. We intend to align with the Managed kdb Insights roadmap, adding additional functionality to the TorQ implementation as new features become available. The processes available in our first iteration of TorQ for Amazon FinSpace with Managed kdb Insights are the following: Discovery: Processes use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections - it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections. Historical Database: The HDB holds data from before the current period. It is read only and used for querying all historical data. Data is stored in int partitions (one per period) and can be queried through the gateway process. Real-time Database: The RDB subscribes and captures all data from the feed handler throughout the current period and store it in memory for query or real-time processing. Feed Handler: The feed handler acts as a preparation stage for the data, transforming the data into kdb+ format and writing it to our RDB. Gateway: The gateway acts as a single interface point that separates the end user from the configuration of underlying databases. You don't need to know where data is stored, and you don't need to make multiple requests to retrieve it. It can access a single process, or join data across multiple processes. It also does load balancing and implements a level of resilience by hiding back-end process failure from clients. These features allow us to store real-time and historical data and make it available to users.","title":"TorQ with Managed kdb Insights"},{"location":"03-torqwithmanagedkdbinsights/#notable-differences-within-this-reduced-version-of-torq-in-comparison-to-normal-torq","text":"","title":"Notable Differences within this reduced version of TorQ in comparison to normal TorQ"},{"location":"03-torqwithmanagedkdbinsights/#envq","text":"The entry point script is env.q which sets initial environment variables and loads the main torq.q script. This is used because Managed kdb Insights does not allow setting environment variables or running shell scripts at startup. The \u201cenv.q\u201d file is the first of our files loaded in and it then specifies based on start-up parameters what to load in next. It also references and connects to the setup created inside Managed kdb Insights . For example, the database name for your AWS environment is referenced inside of this file.","title":"env.q"},{"location":"03-torqwithmanagedkdbinsights/#loading-codehdbs","text":"Generally the main process code file (e.g. code/processes/discovery.q ) is passed as a command line parameter with the -load flag. We can't do this in Managed kdb Insights as filepaths are not allowed as command line parameters (yet). To work around this, we include a .proc.params[`load] variable in the settings file. We use the same approach for loading data directories into HDBs.","title":"Loading code/hdbs"},{"location":"04-prerequisites/","text":"Prerequisites An AWS account with an AdministratorAccess policy to create the Managed kdb resources. A KX insights license applied to our account. If you don\u2019t have one see Activate your Managed kdb Insights license - Amazon FinSpace . Inside a Linux system you will need to download code from the TorQ and TorQ-Amazon-FinSpace-Starter-Pack GitHub repositories - Instructions below. This system will be referred to as your \u2018local repository\u2019 throughout this documentation. - To clone each GitHub repository, please navigate to each respective repository and click the green code button. Downloading the Code TorQ To clone this git repository, please navigate to this repository and copy the ssh link as indicated in the below image. Run the following command in your Linux system to download the code: git clone <your_copied_ssh_link> TorQ Amazon FinSpace Starter Pack To clone this git repository, please navigate to this repository and copy the ssh link as indicated in the below image. Run the following command in your Linux system to download the code: git clone <your_copied_ssh_link> Zip them up Now we will zip these files together: zip -r code.zip TorQ/ TorQ-Amazon-FinSpace-Starter-Pack/ Then upload them to your AWS S3 codebucket: aws s3 cp code.zip s3://<you S3 codebucket name>","title":"Workshop 04 - Prerequisites"},{"location":"04-prerequisites/#prerequisites","text":"An AWS account with an AdministratorAccess policy to create the Managed kdb resources. A KX insights license applied to our account. If you don\u2019t have one see Activate your Managed kdb Insights license - Amazon FinSpace . Inside a Linux system you will need to download code from the TorQ and TorQ-Amazon-FinSpace-Starter-Pack GitHub repositories - Instructions below. This system will be referred to as your \u2018local repository\u2019 throughout this documentation. - To clone each GitHub repository, please navigate to each respective repository and click the green code button.","title":"Prerequisites"},{"location":"04-prerequisites/#downloading-the-code","text":"","title":"Downloading the Code"},{"location":"04-prerequisites/#torq","text":"To clone this git repository, please navigate to this repository and copy the ssh link as indicated in the below image. Run the following command in your Linux system to download the code: git clone <your_copied_ssh_link>","title":"TorQ"},{"location":"04-prerequisites/#torq-amazon-finspace-starter-pack","text":"To clone this git repository, please navigate to this repository and copy the ssh link as indicated in the below image. Run the following command in your Linux system to download the code: git clone <your_copied_ssh_link>","title":"TorQ Amazon FinSpace Starter Pack"},{"location":"04-prerequisites/#zip-them-up","text":"Now we will zip these files together: zip -r code.zip TorQ/ TorQ-Amazon-FinSpace-Starter-Pack/ Then upload them to your AWS S3 codebucket: aws s3 cp code.zip s3://<you S3 codebucket name>","title":"Zip them up"},{"location":"05-terraform/","text":"Terraform The process of setting up a working Managed kdb environment manually can take some time - especially if you are new to AWS. To aid this process we have a Terraform deployment option which should allow you to boot TorQ in Managed kdb Insights in a few simple commands. This Terraform script can be used to deploy an entire environment from scratch. This will include: creating and uploading data to S3 buckets with required policies creating IAM roles creating network and transit gateway as well as deploying clusters. It is split into two modules, one for the environment and one for the clusters. This makes the directory more organised and cluster deployments easier to manage. The cluster module is still dependent on the environment module as it will import some variables from here that are needed for cluster creation.","title":"Workshop 05 - Terraform"},{"location":"05-terraform/#terraform","text":"The process of setting up a working Managed kdb environment manually can take some time - especially if you are new to AWS. To aid this process we have a Terraform deployment option which should allow you to boot TorQ in Managed kdb Insights in a few simple commands. This Terraform script can be used to deploy an entire environment from scratch. This will include: creating and uploading data to S3 buckets with required policies creating IAM roles creating network and transit gateway as well as deploying clusters. It is split into two modules, one for the environment and one for the clusters. This makes the directory more organised and cluster deployments easier to manage. The cluster module is still dependent on the environment module as it will import some variables from here that are needed for cluster creation.","title":"Terraform"},{"location":"06-clustercreation/","text":"Creating TorQ Clusters To create a cluster, firstly select your kdb environment: Then select the \u201cClusters\u201d tab, and either of the \u201cCreate Cluster\u201d buttons: Discovery Cluster Set the cluster type to \u201cGeneral Purpose\u201d, also known as \"GP\". Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. discovery1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select \"Browse S3\", search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype discovery procname discovery1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. Leave everything as blank and move on to the next page. Check the entered information in the review page, then select \"create cluster\". Real-Time Database (RDB) Cluster Set the cluster type to \u201cRDB\". Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. rdb1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select \"Browse S3\", search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype rdb procname rdb1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. Select your \"Database name\" from the dropdown menu in the \"Savedown database configuration\" section. Select SDS01 from the \"Savedown volume type\" dropdown menu. Enter an amount (in GiB) of required \"Savedown volume\". 10 for this MVP. Select next to go to the next page. Check the entered information in the review page, then select \"create cluster\". Historical Database (HDB) Cluster Set the cluster type to \"HDB\". Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. hdb1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Auto-scaling and Tags as empty and select next to go to the next page. Select \"Browse S3\", search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype hdb procname hdb1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. For \"Database name\" select your database from the dropdown menu. Changeset will autoselect at this point. Select \"No caching\". Select next to go to the next page. Check the entered information in the review page, then select \"create cluster\". Gateway Cluster Ensure that the Discovery cluster is in a \"Running\" state before creating the Gateway cluster. Set the cluster type to \u201cGateway\". Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. gateway1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select \"Browse S3\", search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype gateway procname gateway1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. This page had no editing options. Select next to go to the next page. Check the entered information in the review page, then select \"create cluster\". Feed Cluster Ensure that the RDB cluster is in a \"Running\" state before creating the Feed cluster. Set the cluster type to \u201cGeneral Purpose\u201d, also known as \"GP\". Choose a name for your cluster. As this is a sample feed and not a \"production\" intended process, please name it feed1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select \"Browse S3\", search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype tradeFeed procname tradeFeed1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. Leave everything as blank and move on to the next page. Check the entered information in the review page, then select \"create cluster\". On Completion When all clusters are up it should look like this: // TODO ZAN update image with true cluster names Errors in cluster creation On cluster creation, most errors will result in your cluster going to a \u201cCreate failed\u201d state. If that is the case you should: Click the cluster name in the \u201cCluster\u201d section of your environment. Scroll down the page and open the \u201cLogs\u201d tab. This should have a message with a more individualised error you can check. If you click the LogStream for an individual log it will take you to AWS CloudWatch where you can filter the messages for keywords or for messages in a certain time window. It is worthwhile checking the logs even for clusters that have been created and searching for terms like err , error or fail .","title":"Workshop 06 - Cluster Creation"},{"location":"06-clustercreation/#creating-torq-clusters","text":"To create a cluster, firstly select your kdb environment: Then select the \u201cClusters\u201d tab, and either of the \u201cCreate Cluster\u201d buttons:","title":"Creating TorQ Clusters"},{"location":"06-clustercreation/#discovery-cluster","text":"Set the cluster type to \u201cGeneral Purpose\u201d, also known as \"GP\". Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. discovery1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select \"Browse S3\", search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype discovery procname discovery1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. Leave everything as blank and move on to the next page. Check the entered information in the review page, then select \"create cluster\".","title":"Discovery Cluster"},{"location":"06-clustercreation/#real-time-database-rdb-cluster","text":"Set the cluster type to \u201cRDB\". Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. rdb1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select \"Browse S3\", search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype rdb procname rdb1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. Select your \"Database name\" from the dropdown menu in the \"Savedown database configuration\" section. Select SDS01 from the \"Savedown volume type\" dropdown menu. Enter an amount (in GiB) of required \"Savedown volume\". 10 for this MVP. Select next to go to the next page. Check the entered information in the review page, then select \"create cluster\".","title":"Real-Time Database (RDB) Cluster"},{"location":"06-clustercreation/#historical-database-hdb-cluster","text":"Set the cluster type to \"HDB\". Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. hdb1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Auto-scaling and Tags as empty and select next to go to the next page. Select \"Browse S3\", search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype hdb procname hdb1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. For \"Database name\" select your database from the dropdown menu. Changeset will autoselect at this point. Select \"No caching\". Select next to go to the next page. Check the entered information in the review page, then select \"create cluster\".","title":"Historical Database (HDB) Cluster"},{"location":"06-clustercreation/#gateway-cluster","text":"Ensure that the Discovery cluster is in a \"Running\" state before creating the Gateway cluster. Set the cluster type to \u201cGateway\". Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. gateway1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select \"Browse S3\", search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype gateway procname gateway1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. This page had no editing options. Select next to go to the next page. Check the entered information in the review page, then select \"create cluster\".","title":"Gateway Cluster"},{"location":"06-clustercreation/#feed-cluster","text":"Ensure that the RDB cluster is in a \"Running\" state before creating the Feed cluster. Set the cluster type to \u201cGeneral Purpose\u201d, also known as \"GP\". Choose a name for your cluster. As this is a sample feed and not a \"production\" intended process, please name it feed1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select \"Browse S3\", search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype tradeFeed procname tradeFeed1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. Leave everything as blank and move on to the next page. Check the entered information in the review page, then select \"create cluster\".","title":"Feed Cluster"},{"location":"06-clustercreation/#on-completion","text":"When all clusters are up it should look like this: // TODO ZAN update image with true cluster names","title":"On Completion"},{"location":"06-clustercreation/#errors-in-cluster-creation","text":"On cluster creation, most errors will result in your cluster going to a \u201cCreate failed\u201d state. If that is the case you should: Click the cluster name in the \u201cCluster\u201d section of your environment. Scroll down the page and open the \u201cLogs\u201d tab. This should have a message with a more individualised error you can check. If you click the LogStream for an individual log it will take you to AWS CloudWatch where you can filter the messages for keywords or for messages in a certain time window. It is worthwhile checking the logs even for clusters that have been created and searching for terms like err , error or fail .","title":"Errors in cluster creation"},{"location":"07-createec2/","text":"Creating and Connect to an EC2 Instance Create a Windows EC2 Instance Navigate to the EC2 service. Select \"launch instance\" to create a new EC2 instance. Most options here can be left as their defaults. Here are the ones that need selected/changing: Select \"Windows\" from the Quick Start options. We need to create a new key pair: Select \"Create new key pair\". Enter a name for your key pair, leave the key pair type as RSA and the file format as .pem . This will download a key file to you PC which you will use to connect to the instance. The network should be in the same VPC as your cluster. Select create a new security group that allows connections from anywhere. - This is only for the purposes of the MVP. For customising see this page on security groups. Adding your new security group to you EC2 Now we need to add the security group of your cluster to your EC2. Navigate to EC2 service. Select \"Instances (running)\". Open your EC2 Instance. Select \"Actions\", \"Security\" then \"Change security groups\". Search and select the security group that is on your clusters, select \"Add security group\" then \"save\". You should now have two security groups, one from the launch wizard, and the one you added manually that is also attached to your clusters. Connecting to your EC2 Instance Open your EC2 Instance. Select connect. Get your password this only needs to be done once. Once you have this password you can skip this step. Select get password. Upload the .pem that was saved to you PC earlier (alternativly you can just paste the contents of this file in the text box). This will return the value of your password. Keep a note of this password as you will need it to connect your EC2. Connect Download the remote desktop file. Run this file and enter the password you recieved above when promted. You should now be connected to the Windows remote desktop.","title":"Workshop 07 - Create EC2"},{"location":"07-createec2/#creating-and-connect-to-an-ec2-instance","text":"","title":"Creating and Connect to an EC2 Instance"},{"location":"07-createec2/#create-a-windows-ec2-instance","text":"Navigate to the EC2 service. Select \"launch instance\" to create a new EC2 instance. Most options here can be left as their defaults. Here are the ones that need selected/changing: Select \"Windows\" from the Quick Start options. We need to create a new key pair: Select \"Create new key pair\". Enter a name for your key pair, leave the key pair type as RSA and the file format as .pem . This will download a key file to you PC which you will use to connect to the instance. The network should be in the same VPC as your cluster. Select create a new security group that allows connections from anywhere. - This is only for the purposes of the MVP. For customising see this page on security groups.","title":"Create a Windows EC2 Instance"},{"location":"07-createec2/#adding-your-new-security-group-to-you-ec2","text":"Now we need to add the security group of your cluster to your EC2. Navigate to EC2 service. Select \"Instances (running)\". Open your EC2 Instance. Select \"Actions\", \"Security\" then \"Change security groups\". Search and select the security group that is on your clusters, select \"Add security group\" then \"save\". You should now have two security groups, one from the launch wizard, and the one you added manually that is also attached to your clusters.","title":"Adding your new security group to you EC2"},{"location":"07-createec2/#connecting-to-your-ec2-instance","text":"Open your EC2 Instance. Select connect.","title":"Connecting to your EC2 Instance"},{"location":"07-createec2/#get-your-password","text":"this only needs to be done once. Once you have this password you can skip this step. Select get password. Upload the .pem that was saved to you PC earlier (alternativly you can just paste the contents of this file in the text box). This will return the value of your password. Keep a note of this password as you will need it to connect your EC2.","title":"Get your password"},{"location":"07-createec2/#connect","text":"Download the remote desktop file. Run this file and enter the password you recieved above when promted. You should now be connected to the Windows remote desktop.","title":"Connect"},{"location":"08-clusterconnectionstring/","text":"Cluster Connection String Create a user In your kdb environment, go to the Users tab and select Add user. Give it a name and select the IAM role you created above. Generate Connection String On the users tab, copy the links for IAM role and User ARN for the user. Navigate to CloudShell. Replace <ARN_COPIED_FROM_ABOVE> with the IAM Role copied above and run the following (this will not return anything): export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" \\ $(aws sts assume-role \\ --role-arn <ARN_COPIED_FROM_ABOVE> \\ --role-session-name \"connect-to-finTorq\" \\ --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\ --output text)) This lets you assume the role that you have just created by taking the values returned from the aws sts assume-role command and setting them in your AWS_ACCESS_KEY_ID... etc environment variables. NOTE - if you need to switch back to your own user within the CloudShell, you will need to run unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN to unset these environment variables. Copy your kdb Environment Id: Replace <YOUR_KDB_ENVIRONMENT_ID> with your kdb environment ID, <USER_ARN_COPIED_ABOVE> with the User ARN, and <NAME_OF_CLUSTER> with the name of the cluster you want to connect to. Run the following: aws finspace get-kx-connection-string --environment-id <YOUR_KDB_ENVIRONMENT_ID> --user-arn <USER_ARN_COPIED_ABOVE> --cluster-name <NAME_OF_CLUSTER> This will return a large connection string which can be used to connect to your cluster.","title":"Workshop 08 - Cluster Connection String"},{"location":"08-clusterconnectionstring/#cluster-connection-string","text":"","title":"Cluster Connection String"},{"location":"08-clusterconnectionstring/#create-a-user","text":"In your kdb environment, go to the Users tab and select Add user. Give it a name and select the IAM role you created above.","title":"Create a user"},{"location":"08-clusterconnectionstring/#generate-connection-string","text":"On the users tab, copy the links for IAM role and User ARN for the user. Navigate to CloudShell. Replace <ARN_COPIED_FROM_ABOVE> with the IAM Role copied above and run the following (this will not return anything): export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" \\ $(aws sts assume-role \\ --role-arn <ARN_COPIED_FROM_ABOVE> \\ --role-session-name \"connect-to-finTorq\" \\ --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\ --output text)) This lets you assume the role that you have just created by taking the values returned from the aws sts assume-role command and setting them in your AWS_ACCESS_KEY_ID... etc environment variables. NOTE - if you need to switch back to your own user within the CloudShell, you will need to run unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN to unset these environment variables. Copy your kdb Environment Id: Replace <YOUR_KDB_ENVIRONMENT_ID> with your kdb environment ID, <USER_ARN_COPIED_ABOVE> with the User ARN, and <NAME_OF_CLUSTER> with the name of the cluster you want to connect to. Run the following: aws finspace get-kx-connection-string --environment-id <YOUR_KDB_ENVIRONMENT_ID> --user-arn <USER_ARN_COPIED_ABOVE> --cluster-name <NAME_OF_CLUSTER> This will return a large connection string which can be used to connect to your cluster.","title":"Generate Connection String"},{"location":"09-setupqpad/","text":"Setup QPad You must follow these steps from within your EC2 instance. Download QPad Navigate to the qInsightPad website and select the download arrow. Download the relavent version (usually x64). Download the Microsoft C++ 2010 service pack (there is a specific DLL from this that you need that is usually installed on Windows machines). Select the relavent version (usually x64). Run the file that was just downloaded. Search for \u2018Edit Environment variables\u2019 and add SSL_VERIFY_SERVER=NO as one of them.","title":"Workshop 09 - Setup QPad"},{"location":"09-setupqpad/#setup-qpad","text":"You must follow these steps from within your EC2 instance.","title":"Setup QPad"},{"location":"09-setupqpad/#download-qpad","text":"Navigate to the qInsightPad website and select the download arrow. Download the relavent version (usually x64). Download the Microsoft C++ 2010 service pack (there is a specific DLL from this that you need that is usually installed on Windows machines). Select the relavent version (usually x64). Run the file that was just downloaded. Search for \u2018Edit Environment variables\u2019 and add SSL_VERIFY_SERVER=NO as one of them.","title":"Download QPad"},{"location":"10-connectingusingqpad/","text":"Connecting USing QPad Open qpad, right-click on KDB+ Servers and Add New Server. Copy everything in the connection string into the symbol field except the beginning tcp:// . Click test and then click OK to save. You have 60 minutes from the creation of your connection string before the connection string becomes stale and requires to be re-generated.","title":"Workshop 10 - Connecting Using QPad"},{"location":"10-connectingusingqpad/#connecting-using-qpad","text":"Open qpad, right-click on KDB+ Servers and Add New Server. Copy everything in the connection string into the symbol field except the beginning tcp:// . Click test and then click OK to save. You have 60 minutes from the creation of your connection string before the connection string becomes stale and requires to be re-generated.","title":"Connecting USing QPad"},{"location":"11-runningqueries/","text":"Running Queries Some example queries have been implemented on the RDB and HDB processes. These are defined in $KDBCODE/rdb/examplequeries.q and $KDBCODE/hdb/examplequeries.q . These can be run directly on the processes themselves, or from the gateway which will join the results if querying across processes. To test, connect to the gateway cluster. Example queries are listed below: // From the gateway, run a query on the RDB .gw.syncexec[\"select sum size by sym from trades\";`rdb] // Run a query on the HDB .gw.syncexec[\"select count i by date from trades\";`hdb] // Run a freeform time bucketed query and join the results across the RDB and HDB // Note that this is generally bad practice as the HDB query doesn't contain a date clause .gw.syncexec[\"select sum size, max price by 0D00:05 xbar time from trades where sym=`IBM\";`hdb`rdb] // Run a query across the RDB and HDB which uses a different join function to add the data from both .gw.syncexecj[\"select sum size by sym from trades\";`rdb`hdb;sum] // Run the pre-defined functions - these are implemented to query the RDB and HDB as efficiently as possible // Run a bucketed HLOC query, both as a string and in functional form .gw.syncexec[\"hloc[2015.01.07;.z.d;0D12]\";`hdb`rdb] .gw.syncexec[(`hloc;2015.01.07;.z.d;0D12);`hdb`rdb] // Run a count by sym across a date range, and add the results. // Run both as a string and in functional from .gw.syncexecj[\"countbysym[2015.01.07;.z.d]\";`hdb`rdb;sum] .gw.syncexecj[(`countbysym;2015.01.07;.z.d);`hdb`rdb;sum] // Run a gateway query with a bespoke join function to line up results and compare today's data with historic data .gw.syncexecj[(`countbysym;2015.01.07;.z.d);`hdb`rdb;{(`sym xkey select sym,histavgsize:size%tradecount from x 0) lj `sym xkey select sym,todayavgsize:size%tradecount from x 1}] // Send a query for a process type which doesn't exist .gw.syncexec[\"select count i by date from trades\";`hdb`rubbish] // Send a query which fails .gw.syncexec[\"1+`a\";`hdb]","title":"Workshop 11 - Running Queries"},{"location":"11-runningqueries/#running-queries","text":"Some example queries have been implemented on the RDB and HDB processes. These are defined in $KDBCODE/rdb/examplequeries.q and $KDBCODE/hdb/examplequeries.q . These can be run directly on the processes themselves, or from the gateway which will join the results if querying across processes. To test, connect to the gateway cluster. Example queries are listed below: // From the gateway, run a query on the RDB .gw.syncexec[\"select sum size by sym from trades\";`rdb] // Run a query on the HDB .gw.syncexec[\"select count i by date from trades\";`hdb] // Run a freeform time bucketed query and join the results across the RDB and HDB // Note that this is generally bad practice as the HDB query doesn't contain a date clause .gw.syncexec[\"select sum size, max price by 0D00:05 xbar time from trades where sym=`IBM\";`hdb`rdb] // Run a query across the RDB and HDB which uses a different join function to add the data from both .gw.syncexecj[\"select sum size by sym from trades\";`rdb`hdb;sum] // Run the pre-defined functions - these are implemented to query the RDB and HDB as efficiently as possible // Run a bucketed HLOC query, both as a string and in functional form .gw.syncexec[\"hloc[2015.01.07;.z.d;0D12]\";`hdb`rdb] .gw.syncexec[(`hloc;2015.01.07;.z.d;0D12);`hdb`rdb] // Run a count by sym across a date range, and add the results. // Run both as a string and in functional from .gw.syncexecj[\"countbysym[2015.01.07;.z.d]\";`hdb`rdb;sum] .gw.syncexecj[(`countbysym;2015.01.07;.z.d);`hdb`rdb;sum] // Run a gateway query with a bespoke join function to line up results and compare today's data with historic data .gw.syncexecj[(`countbysym;2015.01.07;.z.d);`hdb`rdb;{(`sym xkey select sym,histavgsize:size%tradecount from x 0) lj `sym xkey select sym,todayavgsize:size%tradecount from x 1}] // Send a query for a process type which doesn't exist .gw.syncexec[\"select count i by date from trades\";`hdb`rubbish] // Send a query which fails .gw.syncexec[\"1+`a\";`hdb]","title":"Running Queries"},{"location":"12-takingitdown/","text":"Taking it Down Deleting clusters From your Kdb environmnet select the cluster you want to delete and select \"Delete\". On the confirmation dialog box, enter confirm then select \u201cDelete\u201d. Deleting your database From your Kdb environment select the \"Databases\" tab, select the database you want to delete and select \"Delete\". On the confirmation dialog box, enter confirm then select \u201cDelete\u201d.","title":"Workshop 12 - Taking it Down"},{"location":"12-takingitdown/#taking-it-down","text":"","title":"Taking it Down"},{"location":"12-takingitdown/#deleting-clusters","text":"From your Kdb environmnet select the cluster you want to delete and select \"Delete\". On the confirmation dialog box, enter confirm then select \u201cDelete\u201d.","title":"Deleting clusters"},{"location":"12-takingitdown/#deleting-your-database","text":"From your Kdb environment select the \"Databases\" tab, select the database you want to delete and select \"Delete\". On the confirmation dialog box, enter confirm then select \u201cDelete\u201d.","title":"Deleting your database"},{"location":"13-Conclustion/","text":"Conclustion Contact us for Further Assistance We hope you found this AWS workshop informative and valuable. If you have any questions or require further assistance on any of the topics covered during this workshop or any related TorQ services, please don't hesitate to reach out to us at Data Intellect .","title":"Workshop 13 - Conclustion"},{"location":"13-Conclustion/#conclustion","text":"Contact us for Further Assistance We hope you found this AWS workshop informative and valuable. If you have any questions or require further assistance on any of the topics covered during this workshop or any related TorQ services, please don't hesitate to reach out to us at Data Intellect .","title":"Conclustion"},{"location":"architecture/","text":"Architecture When porting TorQ to Managed kdb Insights we chose to make a minimum viable product and then build out features on top of this. This decision was made so that clients can get a working AWS solution that includes the essential TorQ features that are supported by Managed kdb Insights. We intend to align with the \u201cAmazon FinSpace with Managed kdb Insights\u201d roadmap, adding additional functionality to the TorQ implementation as new \u201cManaged kdb Insights\u201d features become available. The processes available in our first iteration of TorQ on Managed kdb Insights are the following: Discovery: processes use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections- it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections. Historical Database: the HDB is a database stored in AWS S3. Historical data is stored in date partitions and can be queried through the gateway process. Real-time Database: The RDB receives data from the feed handler throughout the day and stores it in in-memory tables for faster access. Feed Handler: The feed handler acts as a preparation stage for the data, transforming the data into kdb+ format and writing it to our RDB. The current version of code has the feed handler pushing simulated data to the RDB. Gateway: The gateway connects to the RDB and HDB processes and runs queries against them. It can access a single process, or join data across multiple processes. It also does load balancing and implements a level of resilience by hiding back-end process failure from clients. These features allow us to store real-time and historical data and make it available to users.","title":"Architecture"},{"location":"architecture/#architecture","text":"When porting TorQ to Managed kdb Insights we chose to make a minimum viable product and then build out features on top of this. This decision was made so that clients can get a working AWS solution that includes the essential TorQ features that are supported by Managed kdb Insights. We intend to align with the \u201cAmazon FinSpace with Managed kdb Insights\u201d roadmap, adding additional functionality to the TorQ implementation as new \u201cManaged kdb Insights\u201d features become available. The processes available in our first iteration of TorQ on Managed kdb Insights are the following: Discovery: processes use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections- it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections. Historical Database: the HDB is a database stored in AWS S3. Historical data is stored in date partitions and can be queried through the gateway process. Real-time Database: The RDB receives data from the feed handler throughout the day and stores it in in-memory tables for faster access. Feed Handler: The feed handler acts as a preparation stage for the data, transforming the data into kdb+ format and writing it to our RDB. The current version of code has the feed handler pushing simulated data to the RDB. Gateway: The gateway connects to the RDB and HDB processes and runs queries against them. It can access a single process, or join data across multiple processes. It also does load balancing and implements a level of resilience by hiding back-end process failure from clients. These features allow us to store real-time and historical data and make it available to users.","title":"Architecture"},{"location":"gettingstarted/","text":"Getting Started The TorQ Finspace Starter Pack is designed to run on Amazon Managed kdb Insights. It contains an optional small initial database of 260MB. As the system runs, data is fed in and written to a managed kdb database at end of day. Installation and Configuration Prerequisites An AWS account with an AdministratorAccess policy to create the Managed kdb Insights resources. You need a KX insights license applied to our account. If you don\u2019t have one see Activate your Managed kdb Insights license - Amazon FinSpace . Inside a Linux system you will need to download the TorQ and TorQ FinSpace Starter Pack GitHub code repos. Start Up Zip up TorQ and TorQ-Amazon-FinSpace-Starter-Pack and name it code.zip Follow the steps in our Terraform documentation to get your environment and clusters (processes) started. The process of setting up a working Managed kdb Insights environment manually can take some time - especially if you are new to AWS. To aid this process we have a Terraform deployment option which should allow you to boot TorQ in Managed kdb Insights in a few simple commands. This Terraform script can be used to deploy an entire environment from scratch. Including creating and uploading data to s3 buckets with required policies, creating IAM roles, and creating network and transit gateway, as well as deploying clusters. It is split into two modules, one for the environment and one for the clusters - which makes the directory more organised and easier to manage cluster deployment. The cluster module is still dependent on the environment module as it will import some variables from here that are needed for cluster creation. For setting up your environment and/or clusters manually, more details will become available in our AWS workshop which is due to be published December 2023 - January 2024. Check If the System Is Running Below is an example of what running clusters look like. You can find this page by going to the AWS console -> Amazon Finspace -> Kdb Environment -> select your environment -> clusters tab Connecting Create a user to interact with the clusters Create a role with correct permissions Go to IAM -> Roles -> Create role Select AWS Account as the trusted entity type Click \"create policy\" Switch to the JSON view and copy in the following code { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"finspace:ConnectKxCluster\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" }, { \"Effect\": \"Allow\", \"Action\": \"finspace:GetKxConnectionString\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" } ] } Note the ARN should match that of your created cluster, although there doesn\u2019t seem to be anywhere to copy this from directly. Save the policy, go back to the role view and click the refresh button Search for the policy you just created and select it and click next Give a relevant name to your role - you will need to update the Trust Policy, but you don\u2019t seem to be able to do it at this point so just save the role at this point. Once created, search for the role and open it in the IAM console Go to Trust relationships \u2192 Edit trust Policy Enter the following JSON { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"finspace.amazonaws.com\", \"AWS\": \"arn:aws:iam::<ACCOUNT_ID>:root\" }, \"Action\": \"sts:AssumeRole\" } ] } Create a user In your kdb environment, go to the Users tab and click Add user Give it a name and select the IAM role you created above. Generate a connection string You may need to wait a while after creating the role/user to make sure that the permissions have propagated fully. On the users tab, copy the ARN from the IAM role of the user Open the AWS CloudShell and run the following (Be sure to edit the below commands with your environment details) export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" \\ $(aws sts assume-role \\ --role-arn <ARN_COPIED_FROM_ABOVE> \\ --role-session-name \"connect-to-finTorq\" \\ --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\ --output text)) This lets you assume the role that you have just created. Note - if you need to switch back to your own user within CloudShell, you can run: unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN Copy the User ARN into your clipboard run the rollowing command in CloudShell (Be sure to replace the USER_ARN role) Amazon FinSpace get-kx-connection-string --environment-id <YOUR_KDB_ENVIRONMENT_ID> --user-arn <USER_ARN_COPIED_ABOVE> --cluster-name <NAME_OF_CLUSTER> This will return a large connection string which can be used to connect to your cluster. Connect to a cluster Create an EC2 instance To connect, you need to set up an EC2 instance running in the same VPC as your cluster. It also must have the same security group as the cluster - What is Amazon EC2? When creating the instance, under the key-pair section, create a new key pair. Leave the key pair type as RSA and file format as .pem. This will download a key which you will use to connect to the instance. Connecting with PyKx (need to be inside your EC2 instance) Follow the instructions here to get a jupyter notebook running. Update the version of the aws cli on your ec2 instance - Install or update the latest version of the AWS CLI Run aws configure and add your credentials (click on username \u2192 Security Settings \u2192 Create Access key) Make sure you have the user created and configured as above Make sure you run the below command before starting the jupyter notebook or running your python code. export SSL_VERIFY_SERVER=NO Try out the sample notebook from here (modifying your connection details) Troubleshooting All the processes logs can be found in AWS Cloudwatch. In general each process writes three logs: a standard out log, a standard error log and a usage log (the queries which have been run against the process remotely). Check these log files for errors. Errors in cluster creation On cluster creation, most errors will result in your cluster going to a \u201cCreate failed\u201d state. If that is the case you should: Click the cluster name in the \u201cCluster\u201d section of your environment Scroll down the page and open the \u201cLogs\u201d tab. This should have a message with a more individualised error you can check. If you click the LogStream for an individual log it will take you to AWS CloudWatch where you can filter the messages for keywords or for messages in a certain time window. It is worthwhile checking the logs even for clusters that have been created and searching for terms like \u201cerr\u201d, \u201cerror\u201d or \u201cfail\u201d Make It Your Own To customize it for a specific data set update the schema file (code/rdb/schema.q) and replace the feed process with a feed of data from a live system.","title":"Getting Started"},{"location":"gettingstarted/#getting-started","text":"The TorQ Finspace Starter Pack is designed to run on Amazon Managed kdb Insights. It contains an optional small initial database of 260MB. As the system runs, data is fed in and written to a managed kdb database at end of day.","title":"Getting Started"},{"location":"gettingstarted/#installation-and-configuration","text":"","title":"Installation and Configuration"},{"location":"gettingstarted/#prerequisites","text":"An AWS account with an AdministratorAccess policy to create the Managed kdb Insights resources. You need a KX insights license applied to our account. If you don\u2019t have one see Activate your Managed kdb Insights license - Amazon FinSpace . Inside a Linux system you will need to download the TorQ and TorQ FinSpace Starter Pack GitHub code repos.","title":"Prerequisites"},{"location":"gettingstarted/#start-up","text":"Zip up TorQ and TorQ-Amazon-FinSpace-Starter-Pack and name it code.zip Follow the steps in our Terraform documentation to get your environment and clusters (processes) started. The process of setting up a working Managed kdb Insights environment manually can take some time - especially if you are new to AWS. To aid this process we have a Terraform deployment option which should allow you to boot TorQ in Managed kdb Insights in a few simple commands. This Terraform script can be used to deploy an entire environment from scratch. Including creating and uploading data to s3 buckets with required policies, creating IAM roles, and creating network and transit gateway, as well as deploying clusters. It is split into two modules, one for the environment and one for the clusters - which makes the directory more organised and easier to manage cluster deployment. The cluster module is still dependent on the environment module as it will import some variables from here that are needed for cluster creation. For setting up your environment and/or clusters manually, more details will become available in our AWS workshop which is due to be published December 2023 - January 2024.","title":"Start Up"},{"location":"gettingstarted/#check-if-the-system-is-running","text":"Below is an example of what running clusters look like. You can find this page by going to the AWS console -> Amazon Finspace -> Kdb Environment -> select your environment -> clusters tab","title":"Check If the System Is Running"},{"location":"gettingstarted/#connecting","text":"","title":"Connecting"},{"location":"gettingstarted/#create-a-user-to-interact-with-the-clusters","text":"","title":"Create a user to interact with the clusters"},{"location":"gettingstarted/#create-a-role-with-correct-permissions","text":"Go to IAM -> Roles -> Create role Select AWS Account as the trusted entity type Click \"create policy\" Switch to the JSON view and copy in the following code { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"finspace:ConnectKxCluster\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" }, { \"Effect\": \"Allow\", \"Action\": \"finspace:GetKxConnectionString\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" } ] } Note the ARN should match that of your created cluster, although there doesn\u2019t seem to be anywhere to copy this from directly. Save the policy, go back to the role view and click the refresh button Search for the policy you just created and select it and click next Give a relevant name to your role - you will need to update the Trust Policy, but you don\u2019t seem to be able to do it at this point so just save the role at this point. Once created, search for the role and open it in the IAM console Go to Trust relationships \u2192 Edit trust Policy Enter the following JSON { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"finspace.amazonaws.com\", \"AWS\": \"arn:aws:iam::<ACCOUNT_ID>:root\" }, \"Action\": \"sts:AssumeRole\" } ] }","title":"Create a role with correct permissions"},{"location":"gettingstarted/#create-a-user","text":"In your kdb environment, go to the Users tab and click Add user Give it a name and select the IAM role you created above.","title":"Create a user"},{"location":"gettingstarted/#generate-a-connection-string","text":"You may need to wait a while after creating the role/user to make sure that the permissions have propagated fully. On the users tab, copy the ARN from the IAM role of the user Open the AWS CloudShell and run the following (Be sure to edit the below commands with your environment details) export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" \\ $(aws sts assume-role \\ --role-arn <ARN_COPIED_FROM_ABOVE> \\ --role-session-name \"connect-to-finTorq\" \\ --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\ --output text)) This lets you assume the role that you have just created. Note - if you need to switch back to your own user within CloudShell, you can run: unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN Copy the User ARN into your clipboard run the rollowing command in CloudShell (Be sure to replace the USER_ARN role) Amazon FinSpace get-kx-connection-string --environment-id <YOUR_KDB_ENVIRONMENT_ID> --user-arn <USER_ARN_COPIED_ABOVE> --cluster-name <NAME_OF_CLUSTER> This will return a large connection string which can be used to connect to your cluster.","title":"Generate a connection string"},{"location":"gettingstarted/#connect-to-a-cluster","text":"","title":"Connect to a cluster"},{"location":"gettingstarted/#create-an-ec2-instance","text":"To connect, you need to set up an EC2 instance running in the same VPC as your cluster. It also must have the same security group as the cluster - What is Amazon EC2? When creating the instance, under the key-pair section, create a new key pair. Leave the key pair type as RSA and file format as .pem. This will download a key which you will use to connect to the instance.","title":"Create an EC2 instance"},{"location":"gettingstarted/#connecting-with-pykx-need-to-be-inside-your-ec2-instance","text":"Follow the instructions here to get a jupyter notebook running. Update the version of the aws cli on your ec2 instance - Install or update the latest version of the AWS CLI Run aws configure and add your credentials (click on username \u2192 Security Settings \u2192 Create Access key) Make sure you have the user created and configured as above Make sure you run the below command before starting the jupyter notebook or running your python code. export SSL_VERIFY_SERVER=NO Try out the sample notebook from here (modifying your connection details)","title":"Connecting with PyKx (need to be inside your EC2 instance)"},{"location":"gettingstarted/#troubleshooting","text":"All the processes logs can be found in AWS Cloudwatch. In general each process writes three logs: a standard out log, a standard error log and a usage log (the queries which have been run against the process remotely). Check these log files for errors.","title":"Troubleshooting"},{"location":"gettingstarted/#errors-in-cluster-creation","text":"On cluster creation, most errors will result in your cluster going to a \u201cCreate failed\u201d state. If that is the case you should: Click the cluster name in the \u201cCluster\u201d section of your environment Scroll down the page and open the \u201cLogs\u201d tab. This should have a message with a more individualised error you can check. If you click the LogStream for an individual log it will take you to AWS CloudWatch where you can filter the messages for keywords or for messages in a certain time window. It is worthwhile checking the logs even for clusters that have been created and searching for terms like \u201cerr\u201d, \u201cerror\u201d or \u201cfail\u201d","title":"Errors in cluster creation"},{"location":"gettingstarted/#make-it-your-own","text":"To customize it for a specific data set update the schema file (code/rdb/schema.q) and replace the feed process with a feed of data from a live system.","title":"Make It Your Own"},{"location":"terraformdeloyment/","text":"Terraform Deployment for TorQ in Finspace bundle and FinSpace Environment This Terraform setup is designed to deploy and manage a FinSpace environment running a TorQ in Finspace bundle. Prerequisites Ensure that you have the latest version of the AWS CLI installed. (Refer to Resource Link Section) Have the latest version of Terraform installed. (Refer to Resource Link Section) Configure the AWS CLI to your AWS account. (Refer to Resource Link Section) Create a KMS key in the region where you intend to set up your environment. You will also need to edit the key policy to grant FinSpace permissions. Note that FinSpace environments are limited to one per region. Make sure you don't already have an environment set up in the same region. Download this repository along with the latest version of TorQ. This instruction refers to Linux and would only work under the Linux environment. Resource Link For installing AWS CLI AWS Command Line Interface For Connecting AWS CLI to Your AWS Account AWS Sign-In For installing Terraform Install Terraform For detailed Terraform deployment instructions, refer to TorQ in Finspace Deployment / Terraform . How to Use - Initial Deployment (New User Please Follow This Section) (Optional) If you have an HDB you want to migrate to FinSpace, replace the dummy HDB in TorQ-Amazon-FinSpace-Starter-Pack/hdb . Move into the TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments directory; this will be the Terraform working directory from which you should run all terraform commands. Modify variables inside the terraform.tfvars file, such as region name, environment name, database name. You can modify it by replacing the variable name inside of \"Name\" . For example, For the variable on role-name , you can change the variable name by replacing \"finspace-role\" . (Optional) If you have changed the database name from the default finspace-database to any other names, please also edit the env.q file, changing the database name to the new variable that you have set in line 19. Run aws configure in the terminal to set up your access key and secret key from your AWS account. This is needed to connect to your account and use the Terraform deployment. Check our resource link for more instructions on how to find your access key and secret key. From your Terraform working directory which is TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments , run terraform init . If initialized without error, run terraform plan . This will show all resources set to be created or destroyed by Terraform. Run terraform apply to execute this plan. The initial deployment can take approximately 45 minutes, and connection losses can cause errors with deployment, so it's a good idea to run this in nohup . (Using nohup might lead to a higher cost of operating the codes if you are using Terraform from a cloud environment.) Managing Your Infrastructure Once your environment is up and running, you can use this configuration to manage it: Code Updates: If you make any code changes in TorQ or TorQ-Amazon-FinSpace-Starter-Pack and want to apply these to your clusters, rezip these directories and run the Terraform deployment again. This will recreate clusters with the updated code. Cluster Config: If you want to make changes to a cluster's config settings (e.g., node size of the RDB), update this in clusters/rdb.tf and run Terraform again. The RDB will be recreated with the new node size. Delete/Create Clusters: Clusters can be deleted or created individually or all at once from the terraform.tfvars file. To delete a cluster, set its count to 0. To delete all clusters, set create-clusters to 0. log groups and metric filters: These resources are only created if the dependent log groups exists. Update the wdb_log_groups variable in terraform.tfvars to include the names of the log groups of your clusters you wish to monitor. WARNING: this terraform stack will fail inelegantly if you list a name of an unexisting log group. To be amended in future iterations. Basic Commands in Terraform terraform init - Prepare your working directory for other commands terraform validate - Check whether the configuration is valid terraform plan - Show changes required by the current configuration terraform apply - Create or update infrastructure terraform destroy - Destroy previously-created infrastructure For more commands in Terraform, please visit Terraform Command Terraform State Management Terraform maintains a state file that tracks the state of the deployed infrastructure. This state file is crucial for Terraform to understand what resources have been created and to make changes to them. To ensure proper state management: Always store your state files securely, as they may contain sensitive information. Consider using remote state storage, such as Amazon S3, to keep your state files safe and accessible from multiple locations. Avoid manual changes to resources managed by Terraform, as this can lead to inconsistencies between the actual infrastructure and Terraform's state. Deploying With Terraform (User With Existing Infrastructure) For users with existing infrastructure in their AWS account who would like to reuse the same resources for their TorQ in Finspace bundle, you can use import blocks in Terraform. This functionality allows you to import existing infrastructure resources into Terraform, bringing them under Terraform's management. The import block records that Terraform imported the resource and did not create it. After importing, you can optionally remove import blocks from your configuration or leave them as a record of the resource's origin. Once imported, Terraform tracks the resource in your state file. You can then manage the imported resource like any other, updating its attributes and destroying it as part of a standard resource lifecycle. Move into the deployments directory, and you'll see an imports.tf file (currently empty). This imports.tf file is automatically run before Terraform applies any changes to the structure, importing existing structures from your AWS to the deployment system. Terraform Import Block Syntax import { to = aws_instance.example id = \"i-abcd1234\" } resource \"aws_instance\" \"example\" { name = \"hashi\" # (other resource arguments...) } The above import block defines an import of the AWS instance with the ID \"i-abcd1234\" into the aws_instance.example resource in the root module. The import block has the following arguments: to - The instance address this resource will have in your state file. id - A string with the import ID of the resource. `provider`` (optional) - An optional custom resource provider, see The Resource provider Meta-Argument for details. If you do not set the provider argument, Terraform attempts to import from the default provider. The import block's id argument can be a literal string of your resource's import ID, or an expression that evaluates to a string. Terraform needs this import ID to locate the resource you want to import. The import ID must be known at plan time for planning to succeed. If the value of id is only known after apply, terraform plan will fail with an error. The identifier you use for a resource's import ID is resource-specific. You can find the required ID in the provider documentation for the resource you wish to import. Terraform import block Template We have created a Terraform import block template in terraform-deployment/importtemplate.md . In this template, you can select the needed import block and paste it into the imports.tf file within the terraform-deployment/deployments/imports.tf directory. Remember to change the ID to the referring ID of your existing infrastructure. List of AWS Structures that will be created with our Terraform deployment module.environment.data.aws_iam_policy_document.iam-policy module.environment.data.aws_iam_policy_document.s3-code-policy module.environment.data.aws_iam_policy_document.s3-data-policy module.environment.aws_ec2_transit_gateway.test module.environment.aws_finspace_kx_database.database module.environment.aws_finspace_kx_environment.environment module.environment.aws_finspace_kx_user.finspace-user module.environment.aws_iam_policy.finspace-policy module.environment.aws_iam_role.finspace-test-role module.environment.aws_iam_role_policy_attachment.policy_attachment module.environment.aws_s3_bucket.finspace-code-bucket module.environment.aws_s3_bucket.finspace-data-bucket module.environment.aws_s3_bucket_policy.code-policy module.environment.aws_s3_bucket_policy.data-policy module.environment.aws_s3_bucket_public_access_block.code_bucket module.environment.aws_s3_bucket_public_access_block.data_bucket module.environment.aws_s3_bucket_versioning.versioning module.environment.null_resource.create_changeset module.environment.null_resource.upload_hdb module.lambda.data.archive_file.lambda_my_function module.lambda.data.aws_iam_policy_document.assume_events_doc module.lambda.data.aws_iam_policy_document.assume_lambda_doc module.lambda.data.aws_iam_policy_document.assume_states_doc module.lambda.data.aws_iam_policy_document.ec2-permissions-lambda module.lambda.data.aws_iam_policy_document.eventBridge_policy_doc module.lambda.data.aws_iam_policy_document.finspace-extra module.lambda.data.aws_iam_policy_document.lambda_basic_execution module.lambda.data.aws_iam_policy_document.lambda_error_queue_access_policy_doc module.lambda.data.aws_iam_policy_document.lambda_invoke_scoped_access_policy_doc module.lambda.data.aws_iam_policy_document.sns_publish_scoped_access_policy_doc module.lambda.data.aws_iam_policy_document.xray_scoped_access_policy_doc module.lambda.aws_cloudwatch_event_rule.rotateRDB_eventRule module.lambda.aws_cloudwatch_event_rule.rotateWDB_eventRule module.lambda.aws_cloudwatch_event_target.onRotateRDB_target module.lambda.aws_cloudwatch_event_target.onRotateWDB_target module.lambda.aws_iam_policy.eventBridge_policy module.lambda.aws_iam_policy.lambda_basic_policy module.lambda.aws_iam_policy.lambda_ec2_policy module.lambda.aws_iam_policy.lambda_finspace_policy module.lambda.aws_iam_policy.lambda_invoke_scoped_access_policy module.lambda.aws_iam_policy.sns_publish_scoped_access_policy module.lambda.aws_iam_policy.xray_scoped_access_policy module.lambda.aws_iam_role.eventBridge_role module.lambda.aws_iam_role.lambda_errorFormat_execution_role module.lambda.aws_iam_role.lambda_execution_role module.lambda.aws_iam_role.lambda_onConflict_execution_role module.lambda.aws_iam_role.states_execution_role module.lambda.aws_iam_role_policy_attachment.attach1 module.lambda.aws_iam_role_policy_attachment.attach2 module.lambda.aws_iam_role_policy_attachment.attach3 module.lambda.aws_iam_role_policy_attachment.attach_basic_to_errorFormat module.lambda.aws_iam_role_policy_attachment.attach_basic_to_onConflict module.lambda.aws_iam_role_policy_attachment.attach_ec2_policy_to_onConflict module.lambda.aws_iam_role_policy_attachment.attach_eventBridge_policy module.lambda.aws_iam_role_policy_attachment.attach_finspace_policy_to_onConflict module.lambda.aws_iam_role_policy_attachment.attach_lambda_invoke_scoped_access_policy module.lambda.aws_iam_role_policy_attachment.attach_sns_publish_scoped_access_policy module.lambda.aws_iam_role_policy_attachment.attach_xray_scoped_access_policy module.lambda.aws_lambda_function.finSpace-rdb-errorFormat-lambda module.lambda.aws_lambda_function.finSpace-rdb-lambda module.lambda.aws_lambda_function.finSpace-rdb-onConflict-lambda module.lambda.aws_sfn_state_machine.sfn_state_machine module.lambda.aws_sns_topic.lambda_error_topic module.lambda.aws_sns_topic_subscription.lambda_error_email_target[0] module.lambda.aws_sns_topic_subscription.lambda_error_queue_target module.lambda.aws_sqs_queue.lambda_error_queue module.lambda.aws_sqs_queue_policy.lambda_error_queue_access_policy module.lambda.local_file.lambda_configs module.metricfilter.data.aws_cloudwatch_log_group.wdb_log_groups[\"*\"] module.metricfilter.aws_cloudwatch_event_rule.wdb_log_monit_rule[0] module.metricfilter.aws_cloudwatch_event_target.wdb_log_monit_rule_target[0] module.metricfilter.aws_cloudwatch_log_metric_filter.wdb_log_monit[\"*\"] module.metricfilter.aws_cloudwatch_metric_alarm.wdb_log_monit_alarm[0] module.network.aws_internet_gateway.finspace-igw module.network.aws_route.finspace-route module.network.aws_route_table.finspace-route-table module.network.aws_security_group.finspace-security-group module.network.aws_subnet.finspace-subnets[0] module.network.aws_subnet.finspace-subnets[1] module.network.aws_subnet.finspace-subnets[2] module.network.aws_subnet.finspace-subnets[3] module.network.aws_vpc.finspace-vpc References and Documentation For more in-depth information and documentation, explore the following resources: Terraform Documentation AWS Documentation These resources provide detailed information about Terraform and AWS services, best practices, and advanced configurations.","title":"Deploy Using Terraform"},{"location":"terraformdeloyment/#terraform-deployment-for-torq-in-finspace-bundle-and-finspace-environment","text":"This Terraform setup is designed to deploy and manage a FinSpace environment running a TorQ in Finspace bundle.","title":"Terraform Deployment for TorQ in Finspace bundle and FinSpace Environment"},{"location":"terraformdeloyment/#prerequisites","text":"Ensure that you have the latest version of the AWS CLI installed. (Refer to Resource Link Section) Have the latest version of Terraform installed. (Refer to Resource Link Section) Configure the AWS CLI to your AWS account. (Refer to Resource Link Section) Create a KMS key in the region where you intend to set up your environment. You will also need to edit the key policy to grant FinSpace permissions. Note that FinSpace environments are limited to one per region. Make sure you don't already have an environment set up in the same region. Download this repository along with the latest version of TorQ. This instruction refers to Linux and would only work under the Linux environment.","title":"Prerequisites"},{"location":"terraformdeloyment/#resource-link","text":"For installing AWS CLI AWS Command Line Interface For Connecting AWS CLI to Your AWS Account AWS Sign-In For installing Terraform Install Terraform For detailed Terraform deployment instructions, refer to TorQ in Finspace Deployment / Terraform .","title":"Resource Link"},{"location":"terraformdeloyment/#how-to-use-initial-deployment-new-user-please-follow-this-section","text":"(Optional) If you have an HDB you want to migrate to FinSpace, replace the dummy HDB in TorQ-Amazon-FinSpace-Starter-Pack/hdb . Move into the TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments directory; this will be the Terraform working directory from which you should run all terraform commands. Modify variables inside the terraform.tfvars file, such as region name, environment name, database name. You can modify it by replacing the variable name inside of \"Name\" . For example, For the variable on role-name , you can change the variable name by replacing \"finspace-role\" . (Optional) If you have changed the database name from the default finspace-database to any other names, please also edit the env.q file, changing the database name to the new variable that you have set in line 19. Run aws configure in the terminal to set up your access key and secret key from your AWS account. This is needed to connect to your account and use the Terraform deployment. Check our resource link for more instructions on how to find your access key and secret key. From your Terraform working directory which is TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments , run terraform init . If initialized without error, run terraform plan . This will show all resources set to be created or destroyed by Terraform. Run terraform apply to execute this plan. The initial deployment can take approximately 45 minutes, and connection losses can cause errors with deployment, so it's a good idea to run this in nohup . (Using nohup might lead to a higher cost of operating the codes if you are using Terraform from a cloud environment.)","title":"How to Use - Initial Deployment (New User Please Follow This Section)"},{"location":"terraformdeloyment/#managing-your-infrastructure","text":"Once your environment is up and running, you can use this configuration to manage it: Code Updates: If you make any code changes in TorQ or TorQ-Amazon-FinSpace-Starter-Pack and want to apply these to your clusters, rezip these directories and run the Terraform deployment again. This will recreate clusters with the updated code. Cluster Config: If you want to make changes to a cluster's config settings (e.g., node size of the RDB), update this in clusters/rdb.tf and run Terraform again. The RDB will be recreated with the new node size. Delete/Create Clusters: Clusters can be deleted or created individually or all at once from the terraform.tfvars file. To delete a cluster, set its count to 0. To delete all clusters, set create-clusters to 0. log groups and metric filters: These resources are only created if the dependent log groups exists. Update the wdb_log_groups variable in terraform.tfvars to include the names of the log groups of your clusters you wish to monitor. WARNING: this terraform stack will fail inelegantly if you list a name of an unexisting log group. To be amended in future iterations.","title":"Managing Your Infrastructure"},{"location":"terraformdeloyment/#basic-commands-in-terraform","text":"terraform init - Prepare your working directory for other commands terraform validate - Check whether the configuration is valid terraform plan - Show changes required by the current configuration terraform apply - Create or update infrastructure terraform destroy - Destroy previously-created infrastructure For more commands in Terraform, please visit Terraform Command","title":"Basic Commands in Terraform"},{"location":"terraformdeloyment/#terraform-state-management","text":"Terraform maintains a state file that tracks the state of the deployed infrastructure. This state file is crucial for Terraform to understand what resources have been created and to make changes to them. To ensure proper state management: Always store your state files securely, as they may contain sensitive information. Consider using remote state storage, such as Amazon S3, to keep your state files safe and accessible from multiple locations. Avoid manual changes to resources managed by Terraform, as this can lead to inconsistencies between the actual infrastructure and Terraform's state.","title":"Terraform State Management"},{"location":"terraformdeloyment/#deploying-with-terraform-user-with-existing-infrastructure","text":"For users with existing infrastructure in their AWS account who would like to reuse the same resources for their TorQ in Finspace bundle, you can use import blocks in Terraform. This functionality allows you to import existing infrastructure resources into Terraform, bringing them under Terraform's management. The import block records that Terraform imported the resource and did not create it. After importing, you can optionally remove import blocks from your configuration or leave them as a record of the resource's origin. Once imported, Terraform tracks the resource in your state file. You can then manage the imported resource like any other, updating its attributes and destroying it as part of a standard resource lifecycle. Move into the deployments directory, and you'll see an imports.tf file (currently empty). This imports.tf file is automatically run before Terraform applies any changes to the structure, importing existing structures from your AWS to the deployment system.","title":"Deploying With Terraform (User With Existing Infrastructure)"},{"location":"terraformdeloyment/#terraform-import-block-syntax","text":"import { to = aws_instance.example id = \"i-abcd1234\" } resource \"aws_instance\" \"example\" { name = \"hashi\" # (other resource arguments...) } The above import block defines an import of the AWS instance with the ID \"i-abcd1234\" into the aws_instance.example resource in the root module. The import block has the following arguments: to - The instance address this resource will have in your state file. id - A string with the import ID of the resource. `provider`` (optional) - An optional custom resource provider, see The Resource provider Meta-Argument for details. If you do not set the provider argument, Terraform attempts to import from the default provider. The import block's id argument can be a literal string of your resource's import ID, or an expression that evaluates to a string. Terraform needs this import ID to locate the resource you want to import. The import ID must be known at plan time for planning to succeed. If the value of id is only known after apply, terraform plan will fail with an error. The identifier you use for a resource's import ID is resource-specific. You can find the required ID in the provider documentation for the resource you wish to import.","title":"Terraform Import Block Syntax"},{"location":"terraformdeloyment/#terraform-import-block-template","text":"We have created a Terraform import block template in terraform-deployment/importtemplate.md . In this template, you can select the needed import block and paste it into the imports.tf file within the terraform-deployment/deployments/imports.tf directory. Remember to change the ID to the referring ID of your existing infrastructure.","title":"Terraform import block Template"},{"location":"terraformdeloyment/#list-of-aws-structures-that-will-be-created-with-our-terraform-deployment","text":"module.environment.data.aws_iam_policy_document.iam-policy module.environment.data.aws_iam_policy_document.s3-code-policy module.environment.data.aws_iam_policy_document.s3-data-policy module.environment.aws_ec2_transit_gateway.test module.environment.aws_finspace_kx_database.database module.environment.aws_finspace_kx_environment.environment module.environment.aws_finspace_kx_user.finspace-user module.environment.aws_iam_policy.finspace-policy module.environment.aws_iam_role.finspace-test-role module.environment.aws_iam_role_policy_attachment.policy_attachment module.environment.aws_s3_bucket.finspace-code-bucket module.environment.aws_s3_bucket.finspace-data-bucket module.environment.aws_s3_bucket_policy.code-policy module.environment.aws_s3_bucket_policy.data-policy module.environment.aws_s3_bucket_public_access_block.code_bucket module.environment.aws_s3_bucket_public_access_block.data_bucket module.environment.aws_s3_bucket_versioning.versioning module.environment.null_resource.create_changeset module.environment.null_resource.upload_hdb module.lambda.data.archive_file.lambda_my_function module.lambda.data.aws_iam_policy_document.assume_events_doc module.lambda.data.aws_iam_policy_document.assume_lambda_doc module.lambda.data.aws_iam_policy_document.assume_states_doc module.lambda.data.aws_iam_policy_document.ec2-permissions-lambda module.lambda.data.aws_iam_policy_document.eventBridge_policy_doc module.lambda.data.aws_iam_policy_document.finspace-extra module.lambda.data.aws_iam_policy_document.lambda_basic_execution module.lambda.data.aws_iam_policy_document.lambda_error_queue_access_policy_doc module.lambda.data.aws_iam_policy_document.lambda_invoke_scoped_access_policy_doc module.lambda.data.aws_iam_policy_document.sns_publish_scoped_access_policy_doc module.lambda.data.aws_iam_policy_document.xray_scoped_access_policy_doc module.lambda.aws_cloudwatch_event_rule.rotateRDB_eventRule module.lambda.aws_cloudwatch_event_rule.rotateWDB_eventRule module.lambda.aws_cloudwatch_event_target.onRotateRDB_target module.lambda.aws_cloudwatch_event_target.onRotateWDB_target module.lambda.aws_iam_policy.eventBridge_policy module.lambda.aws_iam_policy.lambda_basic_policy module.lambda.aws_iam_policy.lambda_ec2_policy module.lambda.aws_iam_policy.lambda_finspace_policy module.lambda.aws_iam_policy.lambda_invoke_scoped_access_policy module.lambda.aws_iam_policy.sns_publish_scoped_access_policy module.lambda.aws_iam_policy.xray_scoped_access_policy module.lambda.aws_iam_role.eventBridge_role module.lambda.aws_iam_role.lambda_errorFormat_execution_role module.lambda.aws_iam_role.lambda_execution_role module.lambda.aws_iam_role.lambda_onConflict_execution_role module.lambda.aws_iam_role.states_execution_role module.lambda.aws_iam_role_policy_attachment.attach1 module.lambda.aws_iam_role_policy_attachment.attach2 module.lambda.aws_iam_role_policy_attachment.attach3 module.lambda.aws_iam_role_policy_attachment.attach_basic_to_errorFormat module.lambda.aws_iam_role_policy_attachment.attach_basic_to_onConflict module.lambda.aws_iam_role_policy_attachment.attach_ec2_policy_to_onConflict module.lambda.aws_iam_role_policy_attachment.attach_eventBridge_policy module.lambda.aws_iam_role_policy_attachment.attach_finspace_policy_to_onConflict module.lambda.aws_iam_role_policy_attachment.attach_lambda_invoke_scoped_access_policy module.lambda.aws_iam_role_policy_attachment.attach_sns_publish_scoped_access_policy module.lambda.aws_iam_role_policy_attachment.attach_xray_scoped_access_policy module.lambda.aws_lambda_function.finSpace-rdb-errorFormat-lambda module.lambda.aws_lambda_function.finSpace-rdb-lambda module.lambda.aws_lambda_function.finSpace-rdb-onConflict-lambda module.lambda.aws_sfn_state_machine.sfn_state_machine module.lambda.aws_sns_topic.lambda_error_topic module.lambda.aws_sns_topic_subscription.lambda_error_email_target[0] module.lambda.aws_sns_topic_subscription.lambda_error_queue_target module.lambda.aws_sqs_queue.lambda_error_queue module.lambda.aws_sqs_queue_policy.lambda_error_queue_access_policy module.lambda.local_file.lambda_configs module.metricfilter.data.aws_cloudwatch_log_group.wdb_log_groups[\"*\"] module.metricfilter.aws_cloudwatch_event_rule.wdb_log_monit_rule[0] module.metricfilter.aws_cloudwatch_event_target.wdb_log_monit_rule_target[0] module.metricfilter.aws_cloudwatch_log_metric_filter.wdb_log_monit[\"*\"] module.metricfilter.aws_cloudwatch_metric_alarm.wdb_log_monit_alarm[0] module.network.aws_internet_gateway.finspace-igw module.network.aws_route.finspace-route module.network.aws_route_table.finspace-route-table module.network.aws_security_group.finspace-security-group module.network.aws_subnet.finspace-subnets[0] module.network.aws_subnet.finspace-subnets[1] module.network.aws_subnet.finspace-subnets[2] module.network.aws_subnet.finspace-subnets[3] module.network.aws_vpc.finspace-vpc","title":"List of AWS Structures that will be created with our Terraform deployment"},{"location":"terraformdeloyment/#references-and-documentation","text":"For more in-depth information and documentation, explore the following resources: Terraform Documentation AWS Documentation These resources provide detailed information about Terraform and AWS services, best practices, and advanced configurations.","title":"References and Documentation"}]}