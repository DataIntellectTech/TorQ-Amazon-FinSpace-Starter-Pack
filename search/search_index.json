{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TorQ in Managed kdb Insights Demo Pack The purpose of the TorQ on Managed kdb Insights Demo Pack is to set up an example TorQ installation on Managed kdb Insights and to show how applications can be built and deployed on top of the TorQ framework. The example installation contains key features of a production data capture installation, including persistence. The demo pack includes: an example set of historic data a simulated data feed configuration changes for base TorQ start and stop scripts using Terraform Once started, TorQ will generate simulated data and push it into an in-memory real-time database inside the rdb cluster. It will persist this data to a Managed kdb database every day at midnight. The system will operate 24*7. email: torqsupport@dataintellect.com web: www.dataintellect.com","title":"Home"},{"location":"#torq-in-managed-kdb-insights-demo-pack","text":"The purpose of the TorQ on Managed kdb Insights Demo Pack is to set up an example TorQ installation on Managed kdb Insights and to show how applications can be built and deployed on top of the TorQ framework. The example installation contains key features of a production data capture installation, including persistence. The demo pack includes: an example set of historic data a simulated data feed configuration changes for base TorQ start and stop scripts using Terraform Once started, TorQ will generate simulated data and push it into an in-memory real-time database inside the rdb cluster. It will persist this data to a Managed kdb database every day at midnight. The system will operate 24*7. email: torqsupport@dataintellect.com web: www.dataintellect.com","title":"TorQ in Managed kdb Insights Demo Pack"},{"location":"01-whatistorq/","text":"What Is TorQ? Data Intellect TorQ is a comprehensive framework built on top of the kdb+ database system. It serves as the foundation for creating production-ready kdb+ systems by providing core functionality and utilities that allow developers to focus on building application business logic. TorQ incorporates numerous best practices, emphasizing performance, maintainability, supportability and extensibility. Key features of Data Intellect TorQ include process management, code management, configuration management, usage logging, connection management (both incoming and outgoing), access controls, timer extensions, standard output/error logging, error handling, documentation and development tools. Currently TorQ within Managed kdb Insights is a minimum viable product (MVP), meaning that not all TorQ functionality has been implemented but in future iterations more functionality will be added. If you would like to read more information on TorQ and all of its functionality, please follow this link . Additional background on this project is here .","title":"What is TorQ?"},{"location":"01-whatistorq/#what-is-torq","text":"Data Intellect TorQ is a comprehensive framework built on top of the kdb+ database system. It serves as the foundation for creating production-ready kdb+ systems by providing core functionality and utilities that allow developers to focus on building application business logic. TorQ incorporates numerous best practices, emphasizing performance, maintainability, supportability and extensibility. Key features of Data Intellect TorQ include process management, code management, configuration management, usage logging, connection management (both incoming and outgoing), access controls, timer extensions, standard output/error logging, error handling, documentation and development tools. Currently TorQ within Managed kdb Insights is a minimum viable product (MVP), meaning that not all TorQ functionality has been implemented but in future iterations more functionality will be added. If you would like to read more information on TorQ and all of its functionality, please follow this link . Additional background on this project is here .","title":"What Is TorQ?"},{"location":"02-Introduction/","text":"Introduction Goals of the workshop To understand what TorQ is and to know the benefits of using it with Managed kdb Insights. To set up a TorQ stack within Managed kdb Insights, connect to the clusters, and access the data within - both historical and real-time. Show existing TorQ users how to migrate their database into Managed kdb Insights. What are we going to build? We will be building \u201cTorQ for Amazon FinSpace with Managed kdb Insights\u201d, a MVP of TorQ which is leveraging functionality within AWS. In this MVP, although all the TorQ code will be included within your code bucket, we will only be using files which are a necessity for this MVP creation. This will create a working TorQ setup on the cloud through Managed kdb Insights. We are going to do so by replicating the below steps. Creating and setting up a Kdb Environment on Amazon Finspace. Create a General Purpose (GP) cluster for the Discovery process of TorQ. This allows other processes to use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability. Create an RDB cluster. This will allow us to query and store live data from the feed. Create a HDB cluster. This will allow us to query historical data. Create Gateway cluster which acts as the gateway within TorQ. This process allows users to query data within the RDB and HDB processes. Lastly, create another General Purpose (GP) cluster within Managed kdb Insights. This will replicate the feed handler of TorQ, which will normalize and prepare our data into a schema readable by kdb, for the ingestion and population of our tables. All of this culminates in a TorQ production system being hosted on the cloud using five general purpose clusters. This allows users to ingest data, before querying both live and historical data through a gateway and discovery process.","title":"Introduction"},{"location":"02-Introduction/#introduction","text":"","title":"Introduction"},{"location":"02-Introduction/#goals-of-the-workshop","text":"To understand what TorQ is and to know the benefits of using it with Managed kdb Insights. To set up a TorQ stack within Managed kdb Insights, connect to the clusters, and access the data within - both historical and real-time. Show existing TorQ users how to migrate their database into Managed kdb Insights.","title":"Goals of the workshop"},{"location":"02-Introduction/#what-are-we-going-to-build","text":"We will be building \u201cTorQ for Amazon FinSpace with Managed kdb Insights\u201d, a MVP of TorQ which is leveraging functionality within AWS. In this MVP, although all the TorQ code will be included within your code bucket, we will only be using files which are a necessity for this MVP creation. This will create a working TorQ setup on the cloud through Managed kdb Insights. We are going to do so by replicating the below steps. Creating and setting up a Kdb Environment on Amazon Finspace. Create a General Purpose (GP) cluster for the Discovery process of TorQ. This allows other processes to use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability. Create an RDB cluster. This will allow us to query and store live data from the feed. Create a HDB cluster. This will allow us to query historical data. Create Gateway cluster which acts as the gateway within TorQ. This process allows users to query data within the RDB and HDB processes. Lastly, create another General Purpose (GP) cluster within Managed kdb Insights. This will replicate the feed handler of TorQ, which will normalize and prepare our data into a schema readable by kdb, for the ingestion and population of our tables. All of this culminates in a TorQ production system being hosted on the cloud using five general purpose clusters. This allows users to ingest data, before querying both live and historical data through a gateway and discovery process.","title":"What are we going to build?"},{"location":"03-torqwithmanagedkdbinsights/","text":"TorQ with Managed kdb Insights When porting TorQ to AWS we chose to make a minimum viable product and then build out features on top of this to give clients a working AWS solution that includes the essential TorQ features. We intend to align with the Managed kdb Insights roadmap, adding additional functionality to the TorQ implementation as new features become available. The processes available in our first iteration of TorQ for Amazon FinSpace with Managed kdb Insights are the following: Discovery: Processes use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections - it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections. Historical Database: The HDB holds data from before the current day. It is read only and used for querying all historical data. Data is stored in date partitions and can be queried through the gateway process. Note: The HDB is unqueriable for around 5 minutes during EOD (End Of Day) processing. Real-time Database: The RDB subscribes and captures all data from the feed handler throughout the current day and store it in memory for query or real-time processing. Feed Handler: The feed handler acts as a preparation stage for the data, transforming the data into kdb+ format and writing it to our RDB. Gateway: The gateway acts as a single interface point that separates the end user from the configuration of underlying databases. You don't need to know where data is stored, and you don't need to make multiple requests to retrieve it. It can access a single process, or join data across multiple processes. It also does load balancing and implements a level of resilience by hiding back-end process failure from clients. These features allow us to store real-time and historical data and make it available to users. Notable Differences within this reduced version of TorQ in comparison to normal TorQ env.q The entry point script is env.q which sets initial environment variables and loads the main torq.q script. This is used because Managed kdb Insights does not allow setting environment variables or running shell scripts at startup. The \u201cenv.q\u201d file is the first of our files loaded in and it then specifies based on start-up parameters what to load in next. It also references and connects to the setup created inside Managed kdb Insights . For example, the database name for your AWS environment is referenced inside of this file. Loading code/hdbs Generally the main process code file (e.g. code/processes/discovery.q ) is passed as a command line parameter with the -load flag. We can't do this in Managed kdb Insights as filepaths are not allowed as command line parameters (yet). To work around this, we include a .proc.params[`load] variable in the settings file. We use the same approach for loading data directories into HDBs.","title":"TorQ With Managed kdb Insights"},{"location":"03-torqwithmanagedkdbinsights/#torq-with-managed-kdb-insights","text":"When porting TorQ to AWS we chose to make a minimum viable product and then build out features on top of this to give clients a working AWS solution that includes the essential TorQ features. We intend to align with the Managed kdb Insights roadmap, adding additional functionality to the TorQ implementation as new features become available. The processes available in our first iteration of TorQ for Amazon FinSpace with Managed kdb Insights are the following: Discovery: Processes use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections - it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections. Historical Database: The HDB holds data from before the current day. It is read only and used for querying all historical data. Data is stored in date partitions and can be queried through the gateway process. Note: The HDB is unqueriable for around 5 minutes during EOD (End Of Day) processing. Real-time Database: The RDB subscribes and captures all data from the feed handler throughout the current day and store it in memory for query or real-time processing. Feed Handler: The feed handler acts as a preparation stage for the data, transforming the data into kdb+ format and writing it to our RDB. Gateway: The gateway acts as a single interface point that separates the end user from the configuration of underlying databases. You don't need to know where data is stored, and you don't need to make multiple requests to retrieve it. It can access a single process, or join data across multiple processes. It also does load balancing and implements a level of resilience by hiding back-end process failure from clients. These features allow us to store real-time and historical data and make it available to users.","title":"TorQ with Managed kdb Insights"},{"location":"03-torqwithmanagedkdbinsights/#notable-differences-within-this-reduced-version-of-torq-in-comparison-to-normal-torq","text":"","title":"Notable Differences within this reduced version of TorQ in comparison to normal TorQ"},{"location":"03-torqwithmanagedkdbinsights/#envq","text":"The entry point script is env.q which sets initial environment variables and loads the main torq.q script. This is used because Managed kdb Insights does not allow setting environment variables or running shell scripts at startup. The \u201cenv.q\u201d file is the first of our files loaded in and it then specifies based on start-up parameters what to load in next. It also references and connects to the setup created inside Managed kdb Insights . For example, the database name for your AWS environment is referenced inside of this file.","title":"env.q"},{"location":"03-torqwithmanagedkdbinsights/#loading-codehdbs","text":"Generally the main process code file (e.g. code/processes/discovery.q ) is passed as a command line parameter with the -load flag. We can't do this in Managed kdb Insights as filepaths are not allowed as command line parameters (yet). To work around this, we include a .proc.params[`load] variable in the settings file. We use the same approach for loading data directories into HDBs.","title":"Loading code/hdbs"},{"location":"04-prerequisites/","text":"Prerequisites An AWS account with an AdministratorAccess policy to create the Managed kdb resources. A KX insights license applied to your account. If you don\u2019t have one see Activate your Managed kdb Insights license - Amazon FinSpace . Inside a Linux system you will need to download code from the TorQ and TorQ-Amazon-FinSpace-Starter-Pack GitHub repositories - Instructions below. If you are NOT using our Terraform deployment option to create and set up your Kdb Environment, follow this AWS workshop to do so. Downloading the Code TorQ Take note of the latest version of code from the TorQ Latest Release Page - release name are v#.#.# e.g. v1.0.0 Run the following code - ensure you replace <copied_version_name> with the release version you took note of above. git clone --depth 1 --branch <copied_version_name> https://github.com/DataIntellectTech/TorQ.git TorQ Amazon FinSpace Starter Pack Take note of the latest version of code from the TorQ-Amazon-FinSpace-Starter-Pack Latest Release Page - release name are v#.#.# e.g. v1.0.0 Run the following code - ensure you replace <copied_version_name> with the release version you took note of above. git clone --depth 1 --branch <copied_version_name> https://github.com/DataIntellectTech/TorQ-Amazon-FinSpace-Starter-Pack.git Zip them up together Now we will zip these files together: zip -r code.zip TorQ/ TorQ-Amazon-FinSpace-Starter-Pack/ -x \"TorQ*/.git*\" Create and Upload code to S3 (For Non Terraform Deployment Only) Two S3 buckets are required for this setup - one for the code and one for the data Create your S3 bucket by searching for \"S3\" and clicking \"Create Bucket\" Choose the same AWS Region as your AWS Finspace KxEnvirnment Give your bucket a name Unselect the \"Block all public access\" box Leave all other settings as the default Edit the access policy Copy the ARN of your S3 buckets in the console by navigating to your S3 bucket, selecting \"Properties\" Edit the Access policy of both S3 buckets with the JSON document: { \"Version\": \"2012-10-17\", \"Id\": \"FinSpaceServiceAccess\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"finspace.amazonaws.com\" }, \"Action\": [ \"s3:GetObject\", \"s3:GetObjectTagging\", \"s3:GetObjectVersion\" ], \"Resource\": \"<ARN OF BUCKET COPIED EARLIER>/*\", \"Condition\": { \"StringEquals\": { \"aws:SourceAccount\": \"766012286003\" } } }, { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"finspace.amazonaws.com\" }, \"Action\": \"s3:ListBucket\", \"Resource\": \"<ARN OF BUCKET COPIED EARLIER>\", \"Condition\": { \"StringEquals\": { \"aws:SourceAccount\": \"766012286003\" } } } ] } Upload code to S3 code bucket Upload the zip file created earlier to AWS S3 codebucket: aws s3 cp code.zip s3://<you S3 codebucket name> Upload hdb to S3 data bucket Copy the pre-packaged hdb to AWS S3 databucket: aws s3 cp --recursive TorQ-Amazon-FinSpace-Starter-Pack/hdb s3://<your S3 data bucket name>/hdb","title":"Prerequisites"},{"location":"04-prerequisites/#prerequisites","text":"An AWS account with an AdministratorAccess policy to create the Managed kdb resources. A KX insights license applied to your account. If you don\u2019t have one see Activate your Managed kdb Insights license - Amazon FinSpace . Inside a Linux system you will need to download code from the TorQ and TorQ-Amazon-FinSpace-Starter-Pack GitHub repositories - Instructions below. If you are NOT using our Terraform deployment option to create and set up your Kdb Environment, follow this AWS workshop to do so.","title":"Prerequisites"},{"location":"04-prerequisites/#downloading-the-code","text":"","title":"Downloading the Code"},{"location":"04-prerequisites/#torq","text":"Take note of the latest version of code from the TorQ Latest Release Page - release name are v#.#.# e.g. v1.0.0 Run the following code - ensure you replace <copied_version_name> with the release version you took note of above. git clone --depth 1 --branch <copied_version_name> https://github.com/DataIntellectTech/TorQ.git","title":"TorQ"},{"location":"04-prerequisites/#torq-amazon-finspace-starter-pack","text":"Take note of the latest version of code from the TorQ-Amazon-FinSpace-Starter-Pack Latest Release Page - release name are v#.#.# e.g. v1.0.0 Run the following code - ensure you replace <copied_version_name> with the release version you took note of above. git clone --depth 1 --branch <copied_version_name> https://github.com/DataIntellectTech/TorQ-Amazon-FinSpace-Starter-Pack.git","title":"TorQ Amazon FinSpace Starter Pack"},{"location":"04-prerequisites/#zip-them-up-together","text":"Now we will zip these files together: zip -r code.zip TorQ/ TorQ-Amazon-FinSpace-Starter-Pack/ -x \"TorQ*/.git*\"","title":"Zip them up together"},{"location":"04-prerequisites/#create-and-upload-code-to-s3-for-non-terraform-deployment-only","text":"Two S3 buckets are required for this setup - one for the code and one for the data Create your S3 bucket by searching for \"S3\" and clicking \"Create Bucket\" Choose the same AWS Region as your AWS Finspace KxEnvirnment Give your bucket a name Unselect the \"Block all public access\" box Leave all other settings as the default","title":"Create and Upload code to S3 (For Non Terraform Deployment Only)"},{"location":"04-prerequisites/#edit-the-access-policy","text":"Copy the ARN of your S3 buckets in the console by navigating to your S3 bucket, selecting \"Properties\" Edit the Access policy of both S3 buckets with the JSON document: { \"Version\": \"2012-10-17\", \"Id\": \"FinSpaceServiceAccess\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"finspace.amazonaws.com\" }, \"Action\": [ \"s3:GetObject\", \"s3:GetObjectTagging\", \"s3:GetObjectVersion\" ], \"Resource\": \"<ARN OF BUCKET COPIED EARLIER>/*\", \"Condition\": { \"StringEquals\": { \"aws:SourceAccount\": \"766012286003\" } } }, { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"finspace.amazonaws.com\" }, \"Action\": \"s3:ListBucket\", \"Resource\": \"<ARN OF BUCKET COPIED EARLIER>\", \"Condition\": { \"StringEquals\": { \"aws:SourceAccount\": \"766012286003\" } } } ] }","title":"Edit the access policy"},{"location":"04-prerequisites/#upload-code-to-s3-code-bucket","text":"Upload the zip file created earlier to AWS S3 codebucket: aws s3 cp code.zip s3://<you S3 codebucket name>","title":"Upload code to S3 code bucket"},{"location":"04-prerequisites/#upload-hdb-to-s3-data-bucket","text":"Copy the pre-packaged hdb to AWS S3 databucket: aws s3 cp --recursive TorQ-Amazon-FinSpace-Starter-Pack/hdb s3://<your S3 data bucket name>/hdb","title":"Upload hdb to S3 data bucket"},{"location":"05-terraform/","text":"Terraform The process of setting up a working Managed kdb environment manually can take some time - especially if you are new to AWS. To aid this process we have a Terraform deployment option which should allow you to boot TorQ in Managed kdb Insights in a few simple commands. This Terraform script can be used to deploy an entire environment from scratch. This will include: creating and uploading data to S3 buckets with required policies creating IAM roles creating network and transit gateway as well as deploying clusters. It is split into two modules, one for the environment and one for the clusters. This makes the directory more organised and cluster deployments easier to manage. The cluster module is still dependent on the environment module as it will import some variables from here that are needed for cluster creation. This Terraform setup is designed to deploy and manage a Managed kdb Insights environment running a TorQ bundle. Prerequisites Ensure you have followed the standard prerequisite steps to ensure you have the latest versions of code. Have the latest version of the AWS CLI installed . Have the latest version of Terraform installed . Configure the AWS CLI to your AWS account . Create a KMS key in the region where you intend to set up your environment. You will also need to edit the key policy to grant FinSpace permissions. Note that FinSpace environments are limited to one per region. Make sure you don't already have an environment set up in the same region. This instruction refers to Linux and would only work under the Linux environment. Resource Link For detailed Terraform deployment instructions, refer to TorQ in Finspace Deployment / Terraform . How to Use - Initial Deployment New user please continue and follow this section - Users with existing infrastructure, please skip to our existing infrastructure section . (Optional) If you have an HDB you want to migrate to FinSpace, replace the dummy HDB in TorQ-Amazon-FinSpace-Starter-Pack/hdb . Move into the TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments directory; this will be the Terraform working directory from which you should run all terraform commands. Modify variables inside the terraform.tfvars file, such as region name, environment name, database name. You can modify it by replacing the variable name inside of \"Name\" . For example, For the variable on role-name , you can change the variable name by replacing \"finspace-role\" . (Optional) If you have changed the database name from the default finspace-database to any other names, please also edit the env.q file, changing the database name to the new variable that you have set in line 19. Run aws configure in the terminal to set up your access key and secret key from your AWS account. This is needed to connect to your account and use the Terraform deployment. Check our resource link for more instructions on how to find your access key and secret key Prerequisites . From your Terraform working directory which is TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments , run terraform init . If initialized without error, run terraform plan . This will show all resources set to be created or destroyed by Terraform. Run terraform apply to execute this plan. The initial deployment can take approximately 45 minutes, and connection losses can cause errors with deployment, so it's a good idea to run this in nohup . (Using nohup might lead to a higher cost of operating the codes if you are using Terraform from a cloud environment.) Example nohup run: nohup terraform apply -auto-approve > terraform_apply.log 2>&1 & . You can now skip ahead to our Managing Your Infrastructure section How to Use - Deploying With Terraform For Users With Existing Infrastructure For users with existing infrastructure in their AWS account who would like to reuse the same resources for their TorQ in Finspace bundle, you can use import blocks in Terraform. This functionality allows you to import existing infrastructure resources into Terraform, bringing them under Terraform's management. The import block records that Terraform imported the resource and did not create it. After importing, you can optionally remove import blocks from your configuration or leave them as a record of the resource's origin. Once imported, Terraform tracks the resource in your state file. You can then manage the imported resource like any other, updating its attributes and destroying it as part of a standard resource lifecycle. Move into the deployments directory, and you'll see an imports.tf file (currently empty). This imports.tf file is automatically run before Terraform applies any changes to the structure, importing existing structures from your AWS to the deployment system. Terraform Import Block Syntax import { to = aws_instance.example id = \"i-abcd1234\" } resource \"aws_instance\" \"example\" { name = \"hashi\" # (other resource arguments...) } The above import block defines an import of the AWS instance with the ID \"i-abcd1234\" into the aws_instance.example resource in the root module. The import block has the following arguments: Argument Description Example to The instance address this resource will have in your state file. to = aws_instance.example resourse (e.g. id or name ) A string with the import information of the resource. Example 1. id = \"i-abcd1234\" Example 2. name = \"aws/vendedlogs/finspace/myclustername\" provider (optional) An optional custom resource provider. If you do not set the provider argument, Terraform attempts to import from the default provider. See The Resource provider Meta-Argument for details. The import block's ID/name argument can be a literal string of your resource's import ID, or an expression that evaluates to a string. Terraform needs this detail to locate the resource you want to import. The import ID/name must be known at plan time for planning to succeed. If the value of id / name is only known after apply, terraform plan will fail with an error. The identifier you use for a resource's import ID/name is resource-specific. You can find the required ID/name in the provider documentation for the resource you wish to import. Terraform import block Template We have created a Terraform import block template in terraform-deployment/importtemplate.md . In this template, you can select the needed import block and paste it into the imports.tf file within the terraform-deployment/deployments/imports.tf directory. Remember to change the ID to the referring ID of your existing infrastructure. Managing Your Infrastructure Once your environment is up and running, you can use this configuration to manage it: Code Updates: If you make any code changes in TorQ or TorQ-Amazon-FinSpace-Starter-Pack and want to apply these to your clusters, rezip these directories and run the Terraform deployment again. This will recreate clusters with the updated code. Cluster Config: If you want to make changes to a cluster's config settings (e.g., node size of the RDB), update this in clusters/rdb.tf and run Terraform again. The RDB will be recreated with the new node size. Delete/Create Clusters: Clusters can be deleted or created individually or all at once from the terraform.tfvars file. To delete a cluster, set its count to 0. To delete all clusters, set create-clusters to 0. Basic Commands in Terraform terraform init - Prepare your working directory for other commands terraform validate - Check whether the configuration is valid terraform plan - Show changes required by the current configuration terraform apply - Create or update infrastructure terraform destroy - Destroy previously-created infrastructure For more commands in Terraform, please visit Terraform Command Terraform State Management Terraform maintains a state file that tracks the state of the deployed infrastructure. This state file is crucial for Terraform to understand what resources have been created and to make changes to them. To ensure proper state management: Always store your state files securely, as they may contain sensitive information. Consider using remote state storage, such as Amazon S3, to keep your state files safe and accessible from multiple locations. Avoid manual changes to resources managed by Terraform, as this can lead to inconsistencies between the actual infrastructure and Terraform's state. List of AWS Structures that will be created with our Terraform deployment module.clusters.aws_finspace_kx_cluster.discovery-cluster[0] module.clusters.aws_finspace_kx_cluster.feed-cluster[0] module.clusters.aws_finspace_kx_cluster.gateway-cluster[0] module.clusters.aws_finspace_kx_cluster.hdb-cluster[0] module.clusters.aws_finspace_kx_cluster.rdb-cluster[0] module.environment.data.aws_iam_policy_document.iam-policy module.environment.data.aws_iam_policy_document.s3-code-policy module.environment.data.aws_iam_policy_document.s3-data-policy module.environment.aws_ec2_transit_gateway.test module.environment.aws_finspace_kx_database.database module.environment.aws_finspace_kx_environment.environment module.environment.aws_finspace_kx_user.finspace-user module.environment.aws_iam_policy.finspace-policy module.environment.aws_iam_role.finspace-test-role module.environment.aws_iam_role_policy_attachment.policy_attachment module.environment.aws_s3_bucket.finspace-code-bucket module.environment.aws_s3_bucket.finspace-data-bucket module.environment.aws_s3_bucket_policy.code-policy module.environment.aws_s3_bucket_policy.data-policy module.environment.aws_s3_bucket_public_access_block.code_bucket module.environment.aws_s3_bucket_public_access_block.data_bucket module.environment.aws_s3_bucket_versioning.versioning module.environment.null_resource.create_changeset module.environment.null_resource.upload_hdb module.network.aws_internet_gateway.finspace-igw module.network.aws_route.finspace-route module.network.aws_route_table.finspace-route-table module.network.aws_security_group.finspace-security-group module.network.aws_route_table_association.subnet-assocations[0] module.network.aws_route_table_association.subnet-assocations[1] module.network.aws_route_table_association.subnet-assocations[2] module.network.aws_subnet.finspace-subnets[0] module.network.aws_subnet.finspace-subnets[1] module.network.aws_subnet.finspace-subnets[2] module.network.aws_subnet.finspace-subnets[3] module.network.aws_vpc.finspace-vpc References and Documentation For more in-depth information and documentation, explore the following resources: Terraform Documentation AWS Documentation These resources provide detailed information about Terraform and AWS services, best practices, and advanced configurations.","title":"Terraform"},{"location":"05-terraform/#terraform","text":"The process of setting up a working Managed kdb environment manually can take some time - especially if you are new to AWS. To aid this process we have a Terraform deployment option which should allow you to boot TorQ in Managed kdb Insights in a few simple commands. This Terraform script can be used to deploy an entire environment from scratch. This will include: creating and uploading data to S3 buckets with required policies creating IAM roles creating network and transit gateway as well as deploying clusters. It is split into two modules, one for the environment and one for the clusters. This makes the directory more organised and cluster deployments easier to manage. The cluster module is still dependent on the environment module as it will import some variables from here that are needed for cluster creation. This Terraform setup is designed to deploy and manage a Managed kdb Insights environment running a TorQ bundle.","title":"Terraform"},{"location":"05-terraform/#prerequisites","text":"Ensure you have followed the standard prerequisite steps to ensure you have the latest versions of code. Have the latest version of the AWS CLI installed . Have the latest version of Terraform installed . Configure the AWS CLI to your AWS account . Create a KMS key in the region where you intend to set up your environment. You will also need to edit the key policy to grant FinSpace permissions. Note that FinSpace environments are limited to one per region. Make sure you don't already have an environment set up in the same region. This instruction refers to Linux and would only work under the Linux environment.","title":"Prerequisites"},{"location":"05-terraform/#resource-link","text":"For detailed Terraform deployment instructions, refer to TorQ in Finspace Deployment / Terraform .","title":"Resource Link"},{"location":"05-terraform/#how-to-use-initial-deployment","text":"New user please continue and follow this section - Users with existing infrastructure, please skip to our existing infrastructure section . (Optional) If you have an HDB you want to migrate to FinSpace, replace the dummy HDB in TorQ-Amazon-FinSpace-Starter-Pack/hdb . Move into the TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments directory; this will be the Terraform working directory from which you should run all terraform commands. Modify variables inside the terraform.tfvars file, such as region name, environment name, database name. You can modify it by replacing the variable name inside of \"Name\" . For example, For the variable on role-name , you can change the variable name by replacing \"finspace-role\" . (Optional) If you have changed the database name from the default finspace-database to any other names, please also edit the env.q file, changing the database name to the new variable that you have set in line 19. Run aws configure in the terminal to set up your access key and secret key from your AWS account. This is needed to connect to your account and use the Terraform deployment. Check our resource link for more instructions on how to find your access key and secret key Prerequisites . From your Terraform working directory which is TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments , run terraform init . If initialized without error, run terraform plan . This will show all resources set to be created or destroyed by Terraform. Run terraform apply to execute this plan. The initial deployment can take approximately 45 minutes, and connection losses can cause errors with deployment, so it's a good idea to run this in nohup . (Using nohup might lead to a higher cost of operating the codes if you are using Terraform from a cloud environment.) Example nohup run: nohup terraform apply -auto-approve > terraform_apply.log 2>&1 & . You can now skip ahead to our Managing Your Infrastructure section","title":"How to Use - Initial Deployment"},{"location":"05-terraform/#how-to-use-deploying-with-terraform-for-users-with-existing-infrastructure","text":"For users with existing infrastructure in their AWS account who would like to reuse the same resources for their TorQ in Finspace bundle, you can use import blocks in Terraform. This functionality allows you to import existing infrastructure resources into Terraform, bringing them under Terraform's management. The import block records that Terraform imported the resource and did not create it. After importing, you can optionally remove import blocks from your configuration or leave them as a record of the resource's origin. Once imported, Terraform tracks the resource in your state file. You can then manage the imported resource like any other, updating its attributes and destroying it as part of a standard resource lifecycle. Move into the deployments directory, and you'll see an imports.tf file (currently empty). This imports.tf file is automatically run before Terraform applies any changes to the structure, importing existing structures from your AWS to the deployment system.","title":"How to Use - Deploying With Terraform For Users With Existing Infrastructure"},{"location":"05-terraform/#terraform-import-block-syntax","text":"import { to = aws_instance.example id = \"i-abcd1234\" } resource \"aws_instance\" \"example\" { name = \"hashi\" # (other resource arguments...) } The above import block defines an import of the AWS instance with the ID \"i-abcd1234\" into the aws_instance.example resource in the root module. The import block has the following arguments: Argument Description Example to The instance address this resource will have in your state file. to = aws_instance.example resourse (e.g. id or name ) A string with the import information of the resource. Example 1. id = \"i-abcd1234\" Example 2. name = \"aws/vendedlogs/finspace/myclustername\" provider (optional) An optional custom resource provider. If you do not set the provider argument, Terraform attempts to import from the default provider. See The Resource provider Meta-Argument for details. The import block's ID/name argument can be a literal string of your resource's import ID, or an expression that evaluates to a string. Terraform needs this detail to locate the resource you want to import. The import ID/name must be known at plan time for planning to succeed. If the value of id / name is only known after apply, terraform plan will fail with an error. The identifier you use for a resource's import ID/name is resource-specific. You can find the required ID/name in the provider documentation for the resource you wish to import.","title":"Terraform Import Block Syntax"},{"location":"05-terraform/#terraform-import-block-template","text":"We have created a Terraform import block template in terraform-deployment/importtemplate.md . In this template, you can select the needed import block and paste it into the imports.tf file within the terraform-deployment/deployments/imports.tf directory. Remember to change the ID to the referring ID of your existing infrastructure.","title":"Terraform import block Template"},{"location":"05-terraform/#managing-your-infrastructure","text":"Once your environment is up and running, you can use this configuration to manage it: Code Updates: If you make any code changes in TorQ or TorQ-Amazon-FinSpace-Starter-Pack and want to apply these to your clusters, rezip these directories and run the Terraform deployment again. This will recreate clusters with the updated code. Cluster Config: If you want to make changes to a cluster's config settings (e.g., node size of the RDB), update this in clusters/rdb.tf and run Terraform again. The RDB will be recreated with the new node size. Delete/Create Clusters: Clusters can be deleted or created individually or all at once from the terraform.tfvars file. To delete a cluster, set its count to 0. To delete all clusters, set create-clusters to 0.","title":"Managing Your Infrastructure"},{"location":"05-terraform/#basic-commands-in-terraform","text":"terraform init - Prepare your working directory for other commands terraform validate - Check whether the configuration is valid terraform plan - Show changes required by the current configuration terraform apply - Create or update infrastructure terraform destroy - Destroy previously-created infrastructure For more commands in Terraform, please visit Terraform Command","title":"Basic Commands in Terraform"},{"location":"05-terraform/#terraform-state-management","text":"Terraform maintains a state file that tracks the state of the deployed infrastructure. This state file is crucial for Terraform to understand what resources have been created and to make changes to them. To ensure proper state management: Always store your state files securely, as they may contain sensitive information. Consider using remote state storage, such as Amazon S3, to keep your state files safe and accessible from multiple locations. Avoid manual changes to resources managed by Terraform, as this can lead to inconsistencies between the actual infrastructure and Terraform's state.","title":"Terraform State Management"},{"location":"05-terraform/#list-of-aws-structures-that-will-be-created-with-our-terraform-deployment","text":"module.clusters.aws_finspace_kx_cluster.discovery-cluster[0] module.clusters.aws_finspace_kx_cluster.feed-cluster[0] module.clusters.aws_finspace_kx_cluster.gateway-cluster[0] module.clusters.aws_finspace_kx_cluster.hdb-cluster[0] module.clusters.aws_finspace_kx_cluster.rdb-cluster[0] module.environment.data.aws_iam_policy_document.iam-policy module.environment.data.aws_iam_policy_document.s3-code-policy module.environment.data.aws_iam_policy_document.s3-data-policy module.environment.aws_ec2_transit_gateway.test module.environment.aws_finspace_kx_database.database module.environment.aws_finspace_kx_environment.environment module.environment.aws_finspace_kx_user.finspace-user module.environment.aws_iam_policy.finspace-policy module.environment.aws_iam_role.finspace-test-role module.environment.aws_iam_role_policy_attachment.policy_attachment module.environment.aws_s3_bucket.finspace-code-bucket module.environment.aws_s3_bucket.finspace-data-bucket module.environment.aws_s3_bucket_policy.code-policy module.environment.aws_s3_bucket_policy.data-policy module.environment.aws_s3_bucket_public_access_block.code_bucket module.environment.aws_s3_bucket_public_access_block.data_bucket module.environment.aws_s3_bucket_versioning.versioning module.environment.null_resource.create_changeset module.environment.null_resource.upload_hdb module.network.aws_internet_gateway.finspace-igw module.network.aws_route.finspace-route module.network.aws_route_table.finspace-route-table module.network.aws_security_group.finspace-security-group module.network.aws_route_table_association.subnet-assocations[0] module.network.aws_route_table_association.subnet-assocations[1] module.network.aws_route_table_association.subnet-assocations[2] module.network.aws_subnet.finspace-subnets[0] module.network.aws_subnet.finspace-subnets[1] module.network.aws_subnet.finspace-subnets[2] module.network.aws_subnet.finspace-subnets[3] module.network.aws_vpc.finspace-vpc","title":"List of AWS Structures that will be created with our Terraform deployment"},{"location":"05-terraform/#references-and-documentation","text":"For more in-depth information and documentation, explore the following resources: Terraform Documentation AWS Documentation These resources provide detailed information about Terraform and AWS services, best practices, and advanced configurations.","title":"References and Documentation"},{"location":"06-scalinggroupandvolumecreation/","text":"Creating Kx Managed Insight Scaling Groups, Shared Volumes, and AWS Finspace Dataviews If you have set up your environment using our Terraform deployment option, this page is purely informative. These resources will have be created for you by Terraform. Scaling Groups To create a scaling group through the AWS Console, select your kdb environment and navigate to the \"Kdb scaling groups\" tab: Click the \"Create kdb scaling group\" button Provide a name for the kdb scaling group unique to the kdb environment. You will be asked to choose a Host type as well. Choose \"kx.sg.4xlarge\" or larger host type. Select an Availability Zone make sure it includes your previous created subnet Click the \"Create kdb scaling group\" button when you are happy with the settings. Shared Volume To create a shared volume through the AWS Console, select your kdb environment and navigate to the \"Volumes\" tab: Click the \"Create Volume\" button Provide a name for the volume unique to the kdb environment. You will be asked to choose a Volume Type. For now, \"NAS_1\" is the only option Under NAS_1 configurations, provide details for the hardware type and the amount of disk capacity allocated. Choose either SSD_250 or SSD_1000 for the best performance The size of allocated disk space must be at least 1200 GiB. Choose an Availability Zone. It is recommended that it matches with the Availability Zone you assigned the kdb scaling group to run on Click the \"create volume\" button when you are happy with the settings (Optional) Dataview You must have a shared volume created to perform this step If you plan to run your HDB cluster on a scaling group this step is required. Otherwise, this step is optional. To create a scaling group through the AWS Console, select your kdb environment and navigate to the \"databases\" tab: Select the database that has the changesets appropriate for your use case. To learn more about changesets click this link Navigate to the \"Dataview tab\" and click the \"Create dataview\" button Under \"Dataview details\" provide a name for your dataview that is unique to your kdb environment Choose an Availability Zone. This must match the Availability Zone your kdb scaling group runs on Under \"Changeset update settings\" you have the option of choosing two modes: Auto-update : (Recommended) The dataview will automatically use data from the latest changeset Static : The dataview will use data from a pre-determined changeset id Under \"Segment Configuration\" choose the root path for your \"database path\" and the volume you create in the prior step Click \"Create dataview\" when you are happy with the settings","title":"Scaling Group Creation"},{"location":"06-scalinggroupandvolumecreation/#creating-kx-managed-insight-scaling-groups-shared-volumes-and-aws-finspace-dataviews","text":"If you have set up your environment using our Terraform deployment option, this page is purely informative. These resources will have be created for you by Terraform.","title":"Creating Kx Managed Insight Scaling Groups, Shared Volumes, and AWS Finspace Dataviews"},{"location":"06-scalinggroupandvolumecreation/#scaling-groups","text":"To create a scaling group through the AWS Console, select your kdb environment and navigate to the \"Kdb scaling groups\" tab: Click the \"Create kdb scaling group\" button Provide a name for the kdb scaling group unique to the kdb environment. You will be asked to choose a Host type as well. Choose \"kx.sg.4xlarge\" or larger host type. Select an Availability Zone make sure it includes your previous created subnet Click the \"Create kdb scaling group\" button when you are happy with the settings.","title":"Scaling Groups"},{"location":"06-scalinggroupandvolumecreation/#shared-volume","text":"To create a shared volume through the AWS Console, select your kdb environment and navigate to the \"Volumes\" tab: Click the \"Create Volume\" button Provide a name for the volume unique to the kdb environment. You will be asked to choose a Volume Type. For now, \"NAS_1\" is the only option Under NAS_1 configurations, provide details for the hardware type and the amount of disk capacity allocated. Choose either SSD_250 or SSD_1000 for the best performance The size of allocated disk space must be at least 1200 GiB. Choose an Availability Zone. It is recommended that it matches with the Availability Zone you assigned the kdb scaling group to run on Click the \"create volume\" button when you are happy with the settings","title":"Shared Volume"},{"location":"06-scalinggroupandvolumecreation/#optional-dataview","text":"You must have a shared volume created to perform this step If you plan to run your HDB cluster on a scaling group this step is required. Otherwise, this step is optional. To create a scaling group through the AWS Console, select your kdb environment and navigate to the \"databases\" tab: Select the database that has the changesets appropriate for your use case. To learn more about changesets click this link Navigate to the \"Dataview tab\" and click the \"Create dataview\" button Under \"Dataview details\" provide a name for your dataview that is unique to your kdb environment Choose an Availability Zone. This must match the Availability Zone your kdb scaling group runs on Under \"Changeset update settings\" you have the option of choosing two modes: Auto-update : (Recommended) The dataview will automatically use data from the latest changeset Static : The dataview will use data from a pre-determined changeset id Under \"Segment Configuration\" choose the root path for your \"database path\" and the volume you create in the prior step Click \"Create dataview\" when you are happy with the settings","title":"(Optional) Dataview"},{"location":"07-clustercreation/","text":"Creating TorQ Clusters If you have set up your environment using our Terraform deployment option, this page is purely informative. Your clusters will have been created for you by Terraform. To create a cluster, first select your kdb environment: Then select the Clusters tab, then either of the Create cluster buttons: Prerequisites For these clusters, you will require: A kdb scaling group A database with: A changeset A dataview A volume Discovery Cluster Set the Cluster type to General purpose , also known as \"GP\". Choose a name for your cluster. Note: this name must match your process name ( procname ) added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type ( proctype ) with a number , e.g. discovery1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select Run on kdb scaling group for the Cluster running option. Choose your group in the dropdown in the Kdb scaling group details section. In the Node details section, set the memory reservation per node to the minimum allowed (6 MiB) and leave the rest blank. Leave Tags as empty and select Next to go to the next page. Select Browse S3 , search and select your codebucket and select your code.zip file. Alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select Add command-line argument twice and enter the keys and values in the below table: Key Value proctype discovery procname discovery1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select Next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readily available default), then select next to go to the next page. Leave everything as blank and move on to the next page. Check the entered information in the review page, then select Create cluster . Real-Time Database (RDB) Cluster Set the cluster type to \u201cRDB\". Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. rdb1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select Browse S3 , search and select your codebucket and select your code.zip file. Alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select Add command-line argument twice and enter the keys and values in the below table: Key Value proctype rdb procname rdb1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. Select your \"Database name\" from the dropdown menu in the \"Savedown database configuration\" section. Select SDS01 from the \"Savedown volume type\" dropdown menu. Enter an amount (in GiB) of required \"Savedown volume\". 10 for this MVP. Select next to go to the next page. Check the entered information in the review page, then select Create cluster . Historical Database (HDB) Cluster Set the Cluster type to HDB . Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. hdb1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Auto-scaling and Tags as empty and select next to go to the next page. Select Browse S3 , search and select your codebucket and select your code.zip file. Alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select Add command-line argument twice and enter the keys and values in the below table: Key Value proctype hdb procname hdb1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. For \"Database name\" select your database from the dropdown menu. Changeset will autoselect at this point. Select \"No caching\". Select next to go to the next page. Check the entered information in the review page, then select \"create cluster\". Gateway Cluster Ensure that the Discovery cluster is in a \"Running\" state before creating the Gateway cluster. Set the Cluster type to Gateway . Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. gateway1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select Browse S3 , search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select Add command-line argument twice and enter the keys and values in the below table: Key Value proctype gateway procname gateway1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. This page had no editing options. Select next to go to the next page. Check the entered information in the review page, then select Create cluster . Feed Cluster Ensure that the RDB cluster is in a Running state before creating the Feed cluster. Set the Cluster type to General purpose , also known as \"GP\". Choose a name for your cluster. As this is a sample feed and not a \"production\" intended process, please name it feed1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select Browse S3 , search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype tradeFeed procname tradeFeed1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. Leave everything as blank and move on to the next page. Check the entered information in the review page, then select Create cluster . On Completion When all clusters are up it should look like this: Errors in cluster creation On cluster creation, most errors will result in your cluster going to a Create failed state. If that is the case you should: Click the cluster name in the Clusters section of your environment. Scroll down the page and open the Logs tab. This should have a message with a more individualised error you can check. If you click the LogStream for an individual log it will take you to AWS CloudWatch where you can filter the messages for keywords or for messages in a certain time window. It is worthwhile checking the logs even for clusters that have been created and searching for terms like err , error or fail .","title":"Cluster Creation"},{"location":"07-clustercreation/#creating-torq-clusters","text":"If you have set up your environment using our Terraform deployment option, this page is purely informative. Your clusters will have been created for you by Terraform. To create a cluster, first select your kdb environment: Then select the Clusters tab, then either of the Create cluster buttons:","title":"Creating TorQ Clusters"},{"location":"07-clustercreation/#prerequisites","text":"For these clusters, you will require: A kdb scaling group A database with: A changeset A dataview A volume","title":"Prerequisites"},{"location":"07-clustercreation/#discovery-cluster","text":"Set the Cluster type to General purpose , also known as \"GP\". Choose a name for your cluster. Note: this name must match your process name ( procname ) added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type ( proctype ) with a number , e.g. discovery1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select Run on kdb scaling group for the Cluster running option. Choose your group in the dropdown in the Kdb scaling group details section. In the Node details section, set the memory reservation per node to the minimum allowed (6 MiB) and leave the rest blank. Leave Tags as empty and select Next to go to the next page. Select Browse S3 , search and select your codebucket and select your code.zip file. Alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select Add command-line argument twice and enter the keys and values in the below table: Key Value proctype discovery procname discovery1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select Next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readily available default), then select next to go to the next page. Leave everything as blank and move on to the next page. Check the entered information in the review page, then select Create cluster .","title":"Discovery Cluster"},{"location":"07-clustercreation/#real-time-database-rdb-cluster","text":"Set the cluster type to \u201cRDB\". Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. rdb1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select Browse S3 , search and select your codebucket and select your code.zip file. Alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select Add command-line argument twice and enter the keys and values in the below table: Key Value proctype rdb procname rdb1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. Select your \"Database name\" from the dropdown menu in the \"Savedown database configuration\" section. Select SDS01 from the \"Savedown volume type\" dropdown menu. Enter an amount (in GiB) of required \"Savedown volume\". 10 for this MVP. Select next to go to the next page. Check the entered information in the review page, then select Create cluster .","title":"Real-Time Database (RDB) Cluster"},{"location":"07-clustercreation/#historical-database-hdb-cluster","text":"Set the Cluster type to HDB . Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. hdb1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Auto-scaling and Tags as empty and select next to go to the next page. Select Browse S3 , search and select your codebucket and select your code.zip file. Alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select Add command-line argument twice and enter the keys and values in the below table: Key Value proctype hdb procname hdb1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. For \"Database name\" select your database from the dropdown menu. Changeset will autoselect at this point. Select \"No caching\". Select next to go to the next page. Check the entered information in the review page, then select \"create cluster\".","title":"Historical Database (HDB) Cluster"},{"location":"07-clustercreation/#gateway-cluster","text":"Ensure that the Discovery cluster is in a \"Running\" state before creating the Gateway cluster. Set the Cluster type to Gateway . Choose a name for your cluster. Note, this name must match your process name, procname , added during the next stage of cluster creation - This is due to process connections requiring the cluster name, which is not visible from within the process, but procname is. Our suggestion is to use the process type, proctype , with a number. e.g. gateway1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Enter a node count of \"1\". This will be the number of instances in a cluster. For the MVP only 1 is needed. Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select Browse S3 , search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select Add command-line argument twice and enter the keys and values in the below table: Key Value proctype gateway procname gateway1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. This page had no editing options. Select next to go to the next page. Check the entered information in the review page, then select Create cluster .","title":"Gateway Cluster"},{"location":"07-clustercreation/#feed-cluster","text":"Ensure that the RDB cluster is in a Running state before creating the Feed cluster. Set the Cluster type to General purpose , also known as \"GP\". Choose a name for your cluster. As this is a sample feed and not a \"production\" intended process, please name it feed1 . Select the execution role for the IAM user previously created . The user for all 5 clusters should be the same. This is so that each cluster has the correct permissions. Select \"Run as a dedicated cluster\" for Cluster running option. For the MVP this is the most appropriate option. For customising later, see this page for more details on kdb scaling groups. Select an Availablilty Zone, make sure it includes your previous created subnet . Select your node size. For this MVP we are going to select the smallest. Leave Tags as empty and select next to go to the next page. Select Browse S3 , search and select your codebucket and select your code.zip file. alternatively, you can copy the URL from the codebucket itself. Enter TorQ-Amazon-FinSpace-Starter-Pack/env.q as your initialization script. Select \"Add command-line arguments\" twice and enter the keys and values in the below table: Key Value proctype tradeFeed procname tradeFeed1 This specified initialization script and the command line arguments will set up the necessary environment for your cluster. Select next to go to the next page. Select your previously created VPC ID , Subnets , and Security Groups (we can use the readilty available default), then select next to go to the next page. Leave everything as blank and move on to the next page. Check the entered information in the review page, then select Create cluster .","title":"Feed Cluster"},{"location":"07-clustercreation/#on-completion","text":"When all clusters are up it should look like this:","title":"On Completion"},{"location":"07-clustercreation/#errors-in-cluster-creation","text":"On cluster creation, most errors will result in your cluster going to a Create failed state. If that is the case you should: Click the cluster name in the Clusters section of your environment. Scroll down the page and open the Logs tab. This should have a message with a more individualised error you can check. If you click the LogStream for an individual log it will take you to AWS CloudWatch where you can filter the messages for keywords or for messages in a certain time window. It is worthwhile checking the logs even for clusters that have been created and searching for terms like err , error or fail .","title":"Errors in cluster creation"},{"location":"08-createec2/","text":"Creating and Connect to an EC2 Instance Create a Windows EC2 Instance Navigate to the EC2 service. Select \"launch instance\" to create a new EC2 instance. Most options here can be left as their defaults. Here are the ones that need selected/changing: Select \"Windows\" from the Quick Start options. We need to create a new key pair: Select \"Create new key pair\". Enter a name for your key pair, leave the key pair type as RSA and the file format as .pem . This will download a key file to you PC which you will use to connect to the instance. The network should be in the same VPC as your cluster. Select create a new security group that allows connections from anywhere. - This is only for the purposes of the MVP. For customising see this page on security groups. Adding your new security group to you EC2 Now we need to add the security group of your cluster to your EC2. Navigate to EC2 service. Select \"Instances (running)\". Open your EC2 Instance. Select \"Actions\", \"Security\" then \"Change security groups\". Search and select the security group that is on your clusters, select \"Add security group\" then \"save\". You should now have two security groups, one from the launch wizard, and the one you added manually that is also attached to your clusters. Connecting to your EC2 Instance Open your EC2 Instance. Select connect. Get your password this only needs to be done once. Once you have this password you can skip this step. Select get password. Upload the .pem that was saved to you PC earlier (alternativly you can just paste the contents of this file in the text box). This will return the value of your password. Keep a note of this password as you will need it to connect your EC2. Connect Download the remote desktop file. Run this file and enter the password you recieved above when promted. You should now be connected to the Windows remote desktop.","title":"Create EC2"},{"location":"08-createec2/#creating-and-connect-to-an-ec2-instance","text":"","title":"Creating and Connect to an EC2 Instance"},{"location":"08-createec2/#create-a-windows-ec2-instance","text":"Navigate to the EC2 service. Select \"launch instance\" to create a new EC2 instance. Most options here can be left as their defaults. Here are the ones that need selected/changing: Select \"Windows\" from the Quick Start options. We need to create a new key pair: Select \"Create new key pair\". Enter a name for your key pair, leave the key pair type as RSA and the file format as .pem . This will download a key file to you PC which you will use to connect to the instance. The network should be in the same VPC as your cluster. Select create a new security group that allows connections from anywhere. - This is only for the purposes of the MVP. For customising see this page on security groups.","title":"Create a Windows EC2 Instance"},{"location":"08-createec2/#adding-your-new-security-group-to-you-ec2","text":"Now we need to add the security group of your cluster to your EC2. Navigate to EC2 service. Select \"Instances (running)\". Open your EC2 Instance. Select \"Actions\", \"Security\" then \"Change security groups\". Search and select the security group that is on your clusters, select \"Add security group\" then \"save\". You should now have two security groups, one from the launch wizard, and the one you added manually that is also attached to your clusters.","title":"Adding your new security group to you EC2"},{"location":"08-createec2/#connecting-to-your-ec2-instance","text":"Open your EC2 Instance. Select connect.","title":"Connecting to your EC2 Instance"},{"location":"08-createec2/#get-your-password","text":"this only needs to be done once. Once you have this password you can skip this step. Select get password. Upload the .pem that was saved to you PC earlier (alternativly you can just paste the contents of this file in the text box). This will return the value of your password. Keep a note of this password as you will need it to connect your EC2.","title":"Get your password"},{"location":"08-createec2/#connect","text":"Download the remote desktop file. Run this file and enter the password you recieved above when promted. You should now be connected to the Windows remote desktop.","title":"Connect"},{"location":"09-clusterconnectionstring/","text":"Cluster Connection String Terraform users can skip to our generate connection string section as Terraform will have created and set up your role and user for you. Ensure Your Role has Correct Permissions (If manually set up (not useing Terraform)) Policy The policy you created needs to have at least these permissions (Note the ARN should match that of your created kxEnvironment): { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"finspace:ConnectKxCluster\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" }, { \"Effect\": \"Allow\", \"Action\": \"finspace:GetKxConnectionString\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" } ] } To enable endofday savedowns and your clusters access to your S3 code and data buckets, you will also need the following statements to your policy { \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"finspace:UpdateKxClusterDatabases\" ] \"Resource\": [ \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\", \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxDatabase/*/kxDataview/*\", ] }, { \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:GetObjectTagging\" ] \"Resource\": [ \"<YOUR_S3_CODE_BUCKET_ARN>\", \"<YOUR_S3_CODE_BUCKET_ARN>/*\", \"<YOUR_S3_DATA_BUCKET_ARN>\", \"<YOUR_S3_DATA_BUCKET_ARN>/*\" ] } ] } To find out how to get the ARN of your S3 buckets reference https://dataintellecttech.github.io/TorQ-Finance-Starter-Pack/04-prerequisites.md Role We need to check the Trust Policy of your created role. Search for the role and open it in the IAM console. Go to Trust relationships. Your Trust relationship should have at least these: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"finspace.amazonaws.com\", \"AWS\": \"arn:aws:iam::<ACCOUNT_ID>:root\" }, \"Action\": \"sts:AssumeRole\" } ] } Create a user (If manually set up (not useing Terraform)) In your kdb environment, go to the Users tab and select Add user. Give it a name and select the IAM role you created above. Generate Connection String On the users tab, copy the links for IAM role and User ARN for the user. Navigate to CloudShell. Replace <ARN_COPIED_FROM_ABOVE> with the IAM Role copied above and run the following (this will not return anything): export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" \\ $(aws sts assume-role \\ --role-arn <ARN_COPIED_FROM_ABOVE> \\ --role-session-name \"connect-to-finTorq\" \\ --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\ --output text)) This lets you assume the role that you have just created by taking the values returned from the aws sts assume-role command and setting them in your AWS_ACCESS_KEY_ID... etc environment variables. NOTE - if you need to switch back to your own user within the CloudShell, you will need to run unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN to unset these environment variables. Copy your kdb Environment Id: Replace <YOUR_KDB_ENVIRONMENT_ID> with your kdb environment ID, <USER_ARN_COPIED_ABOVE> with the User ARN, and <NAME_OF_CLUSTER> with the name of the cluster you want to connect to. Run the following: aws finspace get-kx-connection-string --environment-id <YOUR_KDB_ENVIRONMENT_ID> --user-arn <USER_ARN_COPIED_ABOVE> --cluster-name <NAME_OF_CLUSTER> This will return a large connection string which can be used to connect to your cluster.","title":"Cluster Connection String"},{"location":"09-clusterconnectionstring/#cluster-connection-string","text":"Terraform users can skip to our generate connection string section as Terraform will have created and set up your role and user for you.","title":"Cluster Connection String"},{"location":"09-clusterconnectionstring/#ensure-your-role-has-correct-permissions-if-manually-set-up-not-useing-terraform","text":"","title":"Ensure Your Role has Correct Permissions (If manually set up (not useing Terraform))"},{"location":"09-clusterconnectionstring/#policy","text":"The policy you created needs to have at least these permissions (Note the ARN should match that of your created kxEnvironment): { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"finspace:ConnectKxCluster\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" }, { \"Effect\": \"Allow\", \"Action\": \"finspace:GetKxConnectionString\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" } ] } To enable endofday savedowns and your clusters access to your S3 code and data buckets, you will also need the following statements to your policy { \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"finspace:UpdateKxClusterDatabases\" ] \"Resource\": [ \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\", \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxDatabase/*/kxDataview/*\", ] }, { \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:GetObjectTagging\" ] \"Resource\": [ \"<YOUR_S3_CODE_BUCKET_ARN>\", \"<YOUR_S3_CODE_BUCKET_ARN>/*\", \"<YOUR_S3_DATA_BUCKET_ARN>\", \"<YOUR_S3_DATA_BUCKET_ARN>/*\" ] } ] } To find out how to get the ARN of your S3 buckets reference https://dataintellecttech.github.io/TorQ-Finance-Starter-Pack/04-prerequisites.md","title":"Policy"},{"location":"09-clusterconnectionstring/#role","text":"We need to check the Trust Policy of your created role. Search for the role and open it in the IAM console. Go to Trust relationships. Your Trust relationship should have at least these: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"finspace.amazonaws.com\", \"AWS\": \"arn:aws:iam::<ACCOUNT_ID>:root\" }, \"Action\": \"sts:AssumeRole\" } ] }","title":"Role"},{"location":"09-clusterconnectionstring/#create-a-user-if-manually-set-up-not-useing-terraform","text":"In your kdb environment, go to the Users tab and select Add user. Give it a name and select the IAM role you created above.","title":"Create a user (If manually set up (not useing Terraform))"},{"location":"09-clusterconnectionstring/#generate-connection-string","text":"On the users tab, copy the links for IAM role and User ARN for the user. Navigate to CloudShell. Replace <ARN_COPIED_FROM_ABOVE> with the IAM Role copied above and run the following (this will not return anything): export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" \\ $(aws sts assume-role \\ --role-arn <ARN_COPIED_FROM_ABOVE> \\ --role-session-name \"connect-to-finTorq\" \\ --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\ --output text)) This lets you assume the role that you have just created by taking the values returned from the aws sts assume-role command and setting them in your AWS_ACCESS_KEY_ID... etc environment variables. NOTE - if you need to switch back to your own user within the CloudShell, you will need to run unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN to unset these environment variables. Copy your kdb Environment Id: Replace <YOUR_KDB_ENVIRONMENT_ID> with your kdb environment ID, <USER_ARN_COPIED_ABOVE> with the User ARN, and <NAME_OF_CLUSTER> with the name of the cluster you want to connect to. Run the following: aws finspace get-kx-connection-string --environment-id <YOUR_KDB_ENVIRONMENT_ID> --user-arn <USER_ARN_COPIED_ABOVE> --cluster-name <NAME_OF_CLUSTER> This will return a large connection string which can be used to connect to your cluster.","title":"Generate Connection String"},{"location":"10-setupqpad/","text":"Setup QPad You must follow these steps from within your EC2 instance. Download QPad Navigate to the qInsightPad website and select the download arrow. Download the relavent version (usually x64). Download the Microsoft C++ 2010 service pack (there is a specific DLL from this that you need that is usually installed on Windows machines). Select the relavent version (usually x64). Run the file that was just downloaded. Search for \u2018Edit Environment variables\u2019 and add SSL_VERIFY_SERVER=NO as one of them.","title":"Setup QPad"},{"location":"10-setupqpad/#setup-qpad","text":"You must follow these steps from within your EC2 instance.","title":"Setup QPad"},{"location":"10-setupqpad/#download-qpad","text":"Navigate to the qInsightPad website and select the download arrow. Download the relavent version (usually x64). Download the Microsoft C++ 2010 service pack (there is a specific DLL from this that you need that is usually installed on Windows machines). Select the relavent version (usually x64). Run the file that was just downloaded. Search for \u2018Edit Environment variables\u2019 and add SSL_VERIFY_SERVER=NO as one of them.","title":"Download QPad"},{"location":"11-connectingusingqpad/","text":"Connecting Using QPad Open qpad, right-click on KDB+ Servers and Add New Server. Copy everything in the connection string into the symbol field except the beginning tcp:// . Click test and then click OK to save. You have 60 minutes from the creation of your connection string before the connection string becomes stale and requires to be re-generated.","title":"Connecting Using QPad"},{"location":"11-connectingusingqpad/#connecting-using-qpad","text":"Open qpad, right-click on KDB+ Servers and Add New Server. Copy everything in the connection string into the symbol field except the beginning tcp:// . Click test and then click OK to save. You have 60 minutes from the creation of your connection string before the connection string becomes stale and requires to be re-generated.","title":"Connecting Using QPad"},{"location":"12-runningqueries/","text":"Running Queries Some example queries have been implemented on the RDB and HDB processes. These are defined in $KDBCODE/rdb/examplequeries.q and $KDBCODE/hdb/examplequeries.q . These can be run directly on the processes themselves, or from the gateway which will join the results if querying across processes. To test, connect to the gateway cluster. Example queries are listed below: // From the gateway, run a query on the RDB .gw.syncexec[\"select sum size by sym from trades\";`rdb] // Run a query on the HDB .gw.syncexec[\"select count i by date from trades\";`hdb] // Run a freeform time bucketed query and join the results across the RDB and HDB // Note that this is generally bad practice as the HDB query doesn't contain a date clause .gw.syncexec[\"select sum size, max price by 0D00:05 xbar time from trades where sym=`IBM\";`hdb`rdb] // Run a query across the RDB and HDB which uses a different join function to add the data from both .gw.syncexecj[\"select sum size by sym from trades\";`rdb`hdb;sum] // Run the pre-defined functions - these are implemented to query the RDB and HDB as efficiently as possible // Run a bucketed HLOC query, both as a string and in functional form .gw.syncexec[\"hloc[2015.01.07;.z.d;0D12]\";`hdb`rdb] .gw.syncexec[(`hloc;2015.01.07;.z.d;0D12);`hdb`rdb] // Run a count by sym across a date range, and add the results. // Run both as a string and in functional from .gw.syncexecj[\"countbysym[2015.01.07;.z.d]\";`hdb`rdb;sum] .gw.syncexecj[(`countbysym;2015.01.07;.z.d);`hdb`rdb;sum] // Run a gateway query with a bespoke join function to line up results and compare today's data with historic data .gw.syncexecj[(`countbysym;2015.01.07;.z.d);`hdb`rdb;{(`sym xkey select sym,histavgsize:size%tradecount from x 0) lj `sym xkey select sym,todayavgsize:size%tradecount from x 1}] // Send a query for a process type which doesn't exist .gw.syncexec[\"select count i by date from trades\";`hdb`rubbish] // Send a query which fails .gw.syncexec[\"1+`a\";`hdb]","title":"Running Queries"},{"location":"12-runningqueries/#running-queries","text":"Some example queries have been implemented on the RDB and HDB processes. These are defined in $KDBCODE/rdb/examplequeries.q and $KDBCODE/hdb/examplequeries.q . These can be run directly on the processes themselves, or from the gateway which will join the results if querying across processes. To test, connect to the gateway cluster. Example queries are listed below: // From the gateway, run a query on the RDB .gw.syncexec[\"select sum size by sym from trades\";`rdb] // Run a query on the HDB .gw.syncexec[\"select count i by date from trades\";`hdb] // Run a freeform time bucketed query and join the results across the RDB and HDB // Note that this is generally bad practice as the HDB query doesn't contain a date clause .gw.syncexec[\"select sum size, max price by 0D00:05 xbar time from trades where sym=`IBM\";`hdb`rdb] // Run a query across the RDB and HDB which uses a different join function to add the data from both .gw.syncexecj[\"select sum size by sym from trades\";`rdb`hdb;sum] // Run the pre-defined functions - these are implemented to query the RDB and HDB as efficiently as possible // Run a bucketed HLOC query, both as a string and in functional form .gw.syncexec[\"hloc[2015.01.07;.z.d;0D12]\";`hdb`rdb] .gw.syncexec[(`hloc;2015.01.07;.z.d;0D12);`hdb`rdb] // Run a count by sym across a date range, and add the results. // Run both as a string and in functional from .gw.syncexecj[\"countbysym[2015.01.07;.z.d]\";`hdb`rdb;sum] .gw.syncexecj[(`countbysym;2015.01.07;.z.d);`hdb`rdb;sum] // Run a gateway query with a bespoke join function to line up results and compare today's data with historic data .gw.syncexecj[(`countbysym;2015.01.07;.z.d);`hdb`rdb;{(`sym xkey select sym,histavgsize:size%tradecount from x 0) lj `sym xkey select sym,todayavgsize:size%tradecount from x 1}] // Send a query for a process type which doesn't exist .gw.syncexec[\"select count i by date from trades\";`hdb`rubbish] // Send a query which fails .gw.syncexec[\"1+`a\";`hdb]","title":"Running Queries"},{"location":"13-healthcheck/","text":"Check your system is healthy Check If the System Is Running Below is an example of what running clusters look like. You can find this page by going to the AWS console -> Amazon Finspace -> Kdb Environment -> select your environment -> clusters tab. Run some queries Follow steps on our Running Queries page to run some queries on your Gateway to check there are no errors and data is being returned (if there is data available). Check tables are populated/growning If you would like to go into each process to check their health, here are a few details on how. Check Server Connections - .servers.SERVERS Each TorQ process has a table called .servers.SERVERS that is used to track connections. Each process (with the exception of the Feed) should have at least one connection in this table to the discovery process (the discovery will have connections to all live processes). If a handle is shown within this table - column w - you can assume the connection is active. Heartbeats - .hb.hb All processes regularly publish a signal to show it is still active. This signal is called a heartbeat. Heartbeats are received and a summary can be seen stored within the .hb.hb table. The discovery process can be used to view the heartbeat of all processes (with the exception of the Feed).","title":"Health Check"},{"location":"13-healthcheck/#check-your-system-is-healthy","text":"","title":"Check your system is healthy"},{"location":"13-healthcheck/#check-if-the-system-is-running","text":"Below is an example of what running clusters look like. You can find this page by going to the AWS console -> Amazon Finspace -> Kdb Environment -> select your environment -> clusters tab.","title":"Check If the System Is Running"},{"location":"13-healthcheck/#run-some-queries","text":"Follow steps on our Running Queries page to run some queries on your Gateway to check there are no errors and data is being returned (if there is data available).","title":"Run some queries"},{"location":"13-healthcheck/#check-tables-are-populatedgrowning","text":"If you would like to go into each process to check their health, here are a few details on how.","title":"Check tables are populated/growning"},{"location":"13-healthcheck/#check-server-connections-serversservers","text":"Each TorQ process has a table called .servers.SERVERS that is used to track connections. Each process (with the exception of the Feed) should have at least one connection in this table to the discovery process (the discovery will have connections to all live processes). If a handle is shown within this table - column w - you can assume the connection is active.","title":"Check Server Connections - .servers.SERVERS"},{"location":"13-healthcheck/#heartbeats-hbhb","text":"All processes regularly publish a signal to show it is still active. This signal is called a heartbeat. Heartbeats are received and a summary can be seen stored within the .hb.hb table. The discovery process can be used to view the heartbeat of all processes (with the exception of the Feed).","title":"Heartbeats - .hb.hb"},{"location":"14-takingitdown/","text":"Taking it Down Deleting clusters From your Kdb environmnet select the cluster you want to delete and select \"Delete\". On the confirmation dialog box, enter confirm then select \u201cDelete\u201d. Deleting your dataview This step only applies if you created a dataview Select your kdb environment and navigate to the \"databases\" tab. Then select the database your dataview is associated with: Navigate to the \"Dataviews\" tab. Select the circular button to the left of the dataview you want to delete. Click the \"Delete\" button On the confirmation dialog box, type \"confirm\" and then click the \"Delete\" button. Deleting your Shared Volume Select your kdb environment and navigate to the \"Volumes\" tab. Then select the volume you like to delete and click \"Delete\" On the confirmation dialog box, type \"confirm\" and then click the \"Delete\" button. Deleting your Kdb Scaling Group Important: Delete any clusters running on your scaling group before deleting the scaling group Select your kdb environment and navigate to the \"Kdb scaling groups\" tab. Then select the scaling group you like to delete and click \"Delete\" On the confirmation dialog box, type \"confirm\" and then click the \"Delete\" button. Deleting your database From your Kdb environment select the \"Databases\" tab, select the database you want to delete and select \"Delete\". On the confirmation dialog box, enter confirm then select \u201cDelete\u201d.","title":"Taking it Down"},{"location":"14-takingitdown/#taking-it-down","text":"","title":"Taking it Down"},{"location":"14-takingitdown/#deleting-clusters","text":"From your Kdb environmnet select the cluster you want to delete and select \"Delete\". On the confirmation dialog box, enter confirm then select \u201cDelete\u201d.","title":"Deleting clusters"},{"location":"14-takingitdown/#deleting-your-dataview","text":"This step only applies if you created a dataview Select your kdb environment and navigate to the \"databases\" tab. Then select the database your dataview is associated with: Navigate to the \"Dataviews\" tab. Select the circular button to the left of the dataview you want to delete. Click the \"Delete\" button On the confirmation dialog box, type \"confirm\" and then click the \"Delete\" button.","title":"Deleting your dataview"},{"location":"14-takingitdown/#deleting-your-shared-volume","text":"Select your kdb environment and navigate to the \"Volumes\" tab. Then select the volume you like to delete and click \"Delete\" On the confirmation dialog box, type \"confirm\" and then click the \"Delete\" button.","title":"Deleting your Shared Volume"},{"location":"14-takingitdown/#deleting-your-kdb-scaling-group","text":"Important: Delete any clusters running on your scaling group before deleting the scaling group Select your kdb environment and navigate to the \"Kdb scaling groups\" tab. Then select the scaling group you like to delete and click \"Delete\" On the confirmation dialog box, type \"confirm\" and then click the \"Delete\" button.","title":"Deleting your Kdb Scaling Group"},{"location":"14-takingitdown/#deleting-your-database","text":"From your Kdb environment select the \"Databases\" tab, select the database you want to delete and select \"Delete\". On the confirmation dialog box, enter confirm then select \u201cDelete\u201d.","title":"Deleting your database"},{"location":"15-conclusion/","text":"Conclusion Contact us for Further Assistance We hope you found this AWS workshop informative and valuable. If you have any questions or require further assistance on any of the topics covered during this workshop or any related TorQ services, please don't hesitate to reach out to us at Data Intellect . email: torqsupport@dataintellect.com web: www.dataintellect.com","title":"Conclusion"},{"location":"15-conclusion/#conclusion","text":"Contact us for Further Assistance We hope you found this AWS workshop informative and valuable. If you have any questions or require further assistance on any of the topics covered during this workshop or any related TorQ services, please don't hesitate to reach out to us at Data Intellect . email: torqsupport@dataintellect.com web: www.dataintellect.com","title":"Conclusion"}]}