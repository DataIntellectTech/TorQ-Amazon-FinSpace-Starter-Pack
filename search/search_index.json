{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TorQ in Managed kdb Insights Demo Pack The purpose of the TorQ on Managed kdb Insights Demo Pack is to set up an example TorQ installation on Managed kdb Insights and to show how applications can be built and deployed on top of the TorQ framework. The example installation contains key features of a production data capture installation, including persistence. The demo pack includes: an example set of historic data a simulated data feed configuration changes for base TorQ start and stop scripts using Terraform Once started, TorQ will generate simulated data and push it into an in-memory real-time database inside the rdb cluster. It will persist this data to a Managed kdb database every day at midnight. The system will operate 24*7. email: torqsupport@dataintellect.com web: www.dataintellect.com","title":"Home"},{"location":"#torq-in-managed-kdb-insights-demo-pack","text":"The purpose of the TorQ on Managed kdb Insights Demo Pack is to set up an example TorQ installation on Managed kdb Insights and to show how applications can be built and deployed on top of the TorQ framework. The example installation contains key features of a production data capture installation, including persistence. The demo pack includes: an example set of historic data a simulated data feed configuration changes for base TorQ start and stop scripts using Terraform Once started, TorQ will generate simulated data and push it into an in-memory real-time database inside the rdb cluster. It will persist this data to a Managed kdb database every day at midnight. The system will operate 24*7. email: torqsupport@dataintellect.com web: www.dataintellect.com","title":"TorQ in Managed kdb Insights Demo Pack"},{"location":"architecture/","text":"Architecture When porting TorQ to Managed kdb Insights we chose to make a minimum viable product and then build out features on top of this. This decision was made so that clients can get a working AWS solution that includes the essential TorQ features that are supported by Managed kdb Insights. We intend to align with the \u201cAmazon FinSpace with Managed kdb Insights\u201d roadmap, adding additional functionality to the TorQ implementation as new \u201cManaged kdb Insights\u201d features become available. The processes available in our first iteration of TorQ on Managed kdb Insights are the following: Discovery: processes use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections- it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections. Historical Database: the HDB is a database stored in AWS S3. Historical data is stored in date partitions and can be queried through the gateway process. Real-time Database: The RDB receives data from the feed handler throughout the day and stores it in in-memory tables for faster access. Feed Handler: The feed handler acts as a preparation stage for the data, transforming the data into kdb+ format and writing it to our RDB. The current version of code has the feed handler pushing simulated data to the RDB. Gateway: The gateway connects to the RDB and HDB processes and runs queries against them. It can access a single process, or join data across multiple processes. It also does load balancing and implements a level of resilience by hiding back-end process failure from clients. These features allow us to store real-time and historical data and make it available to users.","title":"Architecture"},{"location":"architecture/#architecture","text":"When porting TorQ to Managed kdb Insights we chose to make a minimum viable product and then build out features on top of this. This decision was made so that clients can get a working AWS solution that includes the essential TorQ features that are supported by Managed kdb Insights. We intend to align with the \u201cAmazon FinSpace with Managed kdb Insights\u201d roadmap, adding additional functionality to the TorQ implementation as new \u201cManaged kdb Insights\u201d features become available. The processes available in our first iteration of TorQ on Managed kdb Insights are the following: Discovery: processes use the discovery service to register their own availability, find other processes (by process type), and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections- it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections. Historical Database: the HDB is a database stored in AWS S3. Historical data is stored in date partitions and can be queried through the gateway process. Real-time Database: The RDB receives data from the feed handler throughout the day and stores it in in-memory tables for faster access. Feed Handler: The feed handler acts as a preparation stage for the data, transforming the data into kdb+ format and writing it to our RDB. The current version of code has the feed handler pushing simulated data to the RDB. Gateway: The gateway connects to the RDB and HDB processes and runs queries against them. It can access a single process, or join data across multiple processes. It also does load balancing and implements a level of resilience by hiding back-end process failure from clients. These features allow us to store real-time and historical data and make it available to users.","title":"Architecture"},{"location":"gettingstarted/","text":"Getting Started The TorQ Finspace Starter Pack is designed to run on Amazon Managed kdb Insights. It contains an optional small initial database of 260MB. As the system runs, data is fed in and written to a managed kdb database at end of day. Installation and Configuration Prerequisites An AWS account with an AdministratorAccess policy to create the Managed kdb Insights resources. You need a KX insights license applied to our account. If you don\u2019t have one see Activate your Managed kdb Insights license - Amazon FinSpace . Inside a Linux system you will need to download the TorQ and TorQ FinSpace Starter Pack GitHub code repos. Start Up Zip up TorQ and TorQ-Amazon-FinSpace-Starter-Pack and name it code.zip Follow the steps in our Terraform documentation to get your environment and clusters (processes) started. The process of setting up a working Managed kdb Insights environment manually can take some time - especially if you are new to AWS. To aid this process we have a Terraform deployment option which should allow you to boot TorQ in Managed kdb Insights in a few simple commands. This Terraform script can be used to deploy an entire environment from scratch. Including creating and uploading data to s3 buckets with required policies, creating IAM roles, and creating network and transit gateway, as well as deploying clusters. It is split into two modules, one for the environment and one for the clusters - which makes the directory more organised and easier to manage cluster deployment. The cluster module is still dependent on the environment module as it will import some variables from here that are needed for cluster creation. For setting up your environment and/or clusters manually, more details will become available in our AWS workshop which is due to be published December 2023 - January 2024. Check If the System Is Running Below is an example of what running clusters look like. You can find this page by going to the AWS console -> Amazon Finspace -> Kdb Environment -> select your environment -> clusters tab Connecting Create a user to interact with the clusters Create a role with correct permissions Go to IAM -> Roles -> Create role Select AWS Account as the trusted entity type Click \"create policy\" Switch to the JSON view and copy in the following code { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"finspace:ConnectKxCluster\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" }, { \"Effect\": \"Allow\", \"Action\": \"finspace:GetKxConnectionString\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" } ] } Note the ARN should match that of your created cluster, although there doesn\u2019t seem to be anywhere to copy this from directly. Save the policy, go back to the role view and click the refresh button Search for the policy you just created and select it and click next Give a relevant name to your role - you will need to update the Trust Policy, but you don\u2019t seem to be able to do it at this point so just save the role at this point. Once created, search for the role and open it in the IAM console Go to Trust relationships \u2192 Edit trust Policy Enter the following JSON { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"finspace.amazonaws.com\", \"AWS\": \"arn:aws:iam::<ACCOUNT_ID>:root\" }, \"Action\": \"sts:AssumeRole\" } ] } Create a user In your kdb environment, go to the Users tab and click Add user Give it a name and select the IAM role you created above. Generate a connection string You may need to wait a while after creating the role/user to make sure that the permissions have propagated fully. On the users tab, copy the ARN from the IAM role of the user Open the AWS CloudShell and run the following (Be sure to edit the below commands with your environment details) export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" \\ $(aws sts assume-role \\ --role-arn <ARN_COPIED_FROM_ABOVE> \\ --role-session-name \"connect-to-finTorq\" \\ --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\ --output text)) This lets you assume the role that you have just created. Note - if you need to switch back to your own user within CloudShell, you can run: unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN Copy the User ARN into your clipboard run the rollowing command in CloudShell (Be sure to replace the USER_ARN role) Amazon FinSpace get-kx-connection-string --environment-id <YOUR_KDB_ENVIRONMENT_ID> --user-arn <USER_ARN_COPIED_ABOVE> --cluster-name <NAME_OF_CLUSTER> This will return a large connection string which can be used to connect to your cluster. Connect to a cluster Create an EC2 instance To connect, you need to set up an EC2 instance running in the same VPC as your cluster. It also must have the same security group as the cluster - What is Amazon EC2? When creating the instance, under the key-pair section, create a new key pair. Leave the key pair type as RSA and file format as .pem. This will download a key which you will use to connect to the instance. Connecting with PyKx (need to be inside your EC2 instance) Follow the instructions here to get a jupyter notebook running. Update the version of the aws cli on your ec2 instance - Install or update the latest version of the AWS CLI Run aws configure and add your credentials (click on username \u2192 Security Settings \u2192 Create Access key) Make sure you have the user created and configured as above Make sure you run the below command before starting the jupyter notebook or running your python code. export SSL_VERIFY_SERVER=NO Try out the sample notebook from here (modifying your connection details) Troubleshooting All the processes logs can be found in AWS Cloudwatch. In general each process writes three logs: a standard out log, a standard error log and a usage log (the queries which have been run against the process remotely). Check these log files for errors. Errors in cluster creation On cluster creation, most errors will result in your cluster going to a \u201cCreate failed\u201d state. If that is the case you should: Click the cluster name in the \u201cCluster\u201d section of your environment Scroll down the page and open the \u201cLogs\u201d tab. This should have a message with a more individualised error you can check. If you click the LogStream for an individual log it will take you to AWS CloudWatch where you can filter the messages for keywords or for messages in a certain time window. It is worthwhile checking the logs even for clusters that have been created and searching for terms like \u201cerr\u201d, \u201cerror\u201d or \u201cfail\u201d Make It Your Own To customize it for a specific data set update the schema file (code/rdb/schema.q) and replace the feed process with a feed of data from a live system.","title":"Getting Started"},{"location":"gettingstarted/#getting-started","text":"The TorQ Finspace Starter Pack is designed to run on Amazon Managed kdb Insights. It contains an optional small initial database of 260MB. As the system runs, data is fed in and written to a managed kdb database at end of day.","title":"Getting Started"},{"location":"gettingstarted/#installation-and-configuration","text":"","title":"Installation and Configuration"},{"location":"gettingstarted/#prerequisites","text":"An AWS account with an AdministratorAccess policy to create the Managed kdb Insights resources. You need a KX insights license applied to our account. If you don\u2019t have one see Activate your Managed kdb Insights license - Amazon FinSpace . Inside a Linux system you will need to download the TorQ and TorQ FinSpace Starter Pack GitHub code repos.","title":"Prerequisites"},{"location":"gettingstarted/#start-up","text":"Zip up TorQ and TorQ-Amazon-FinSpace-Starter-Pack and name it code.zip Follow the steps in our Terraform documentation to get your environment and clusters (processes) started. The process of setting up a working Managed kdb Insights environment manually can take some time - especially if you are new to AWS. To aid this process we have a Terraform deployment option which should allow you to boot TorQ in Managed kdb Insights in a few simple commands. This Terraform script can be used to deploy an entire environment from scratch. Including creating and uploading data to s3 buckets with required policies, creating IAM roles, and creating network and transit gateway, as well as deploying clusters. It is split into two modules, one for the environment and one for the clusters - which makes the directory more organised and easier to manage cluster deployment. The cluster module is still dependent on the environment module as it will import some variables from here that are needed for cluster creation. For setting up your environment and/or clusters manually, more details will become available in our AWS workshop which is due to be published December 2023 - January 2024.","title":"Start Up"},{"location":"gettingstarted/#check-if-the-system-is-running","text":"Below is an example of what running clusters look like. You can find this page by going to the AWS console -> Amazon Finspace -> Kdb Environment -> select your environment -> clusters tab","title":"Check If the System Is Running"},{"location":"gettingstarted/#connecting","text":"","title":"Connecting"},{"location":"gettingstarted/#create-a-user-to-interact-with-the-clusters","text":"","title":"Create a user to interact with the clusters"},{"location":"gettingstarted/#create-a-role-with-correct-permissions","text":"Go to IAM -> Roles -> Create role Select AWS Account as the trusted entity type Click \"create policy\" Switch to the JSON view and copy in the following code { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"finspace:ConnectKxCluster\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" }, { \"Effect\": \"Allow\", \"Action\": \"finspace:GetKxConnectionString\", \"Resource\": \"<ENVIRONMENT_ARN_COPIED_FROM_KDB_ENIRONMENT_PAGE>/kxCluster/*\" } ] } Note the ARN should match that of your created cluster, although there doesn\u2019t seem to be anywhere to copy this from directly. Save the policy, go back to the role view and click the refresh button Search for the policy you just created and select it and click next Give a relevant name to your role - you will need to update the Trust Policy, but you don\u2019t seem to be able to do it at this point so just save the role at this point. Once created, search for the role and open it in the IAM console Go to Trust relationships \u2192 Edit trust Policy Enter the following JSON { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"finspace.amazonaws.com\", \"AWS\": \"arn:aws:iam::<ACCOUNT_ID>:root\" }, \"Action\": \"sts:AssumeRole\" } ] }","title":"Create a role with correct permissions"},{"location":"gettingstarted/#create-a-user","text":"In your kdb environment, go to the Users tab and click Add user Give it a name and select the IAM role you created above.","title":"Create a user"},{"location":"gettingstarted/#generate-a-connection-string","text":"You may need to wait a while after creating the role/user to make sure that the permissions have propagated fully. On the users tab, copy the ARN from the IAM role of the user Open the AWS CloudShell and run the following (Be sure to edit the below commands with your environment details) export $(printf \"AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s\" \\ $(aws sts assume-role \\ --role-arn <ARN_COPIED_FROM_ABOVE> \\ --role-session-name \"connect-to-finTorq\" \\ --query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\ --output text)) This lets you assume the role that you have just created. Note - if you need to switch back to your own user within CloudShell, you can run: unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN Copy the User ARN into your clipboard run the rollowing command in CloudShell (Be sure to replace the USER_ARN role) Amazon FinSpace get-kx-connection-string --environment-id <YOUR_KDB_ENVIRONMENT_ID> --user-arn <USER_ARN_COPIED_ABOVE> --cluster-name <NAME_OF_CLUSTER> This will return a large connection string which can be used to connect to your cluster.","title":"Generate a connection string"},{"location":"gettingstarted/#connect-to-a-cluster","text":"","title":"Connect to a cluster"},{"location":"gettingstarted/#create-an-ec2-instance","text":"To connect, you need to set up an EC2 instance running in the same VPC as your cluster. It also must have the same security group as the cluster - What is Amazon EC2? When creating the instance, under the key-pair section, create a new key pair. Leave the key pair type as RSA and file format as .pem. This will download a key which you will use to connect to the instance.","title":"Create an EC2 instance"},{"location":"gettingstarted/#connecting-with-pykx-need-to-be-inside-your-ec2-instance","text":"Follow the instructions here to get a jupyter notebook running. Update the version of the aws cli on your ec2 instance - Install or update the latest version of the AWS CLI Run aws configure and add your credentials (click on username \u2192 Security Settings \u2192 Create Access key) Make sure you have the user created and configured as above Make sure you run the below command before starting the jupyter notebook or running your python code. export SSL_VERIFY_SERVER=NO Try out the sample notebook from here (modifying your connection details)","title":"Connecting with PyKx (need to be inside your EC2 instance)"},{"location":"gettingstarted/#troubleshooting","text":"All the processes logs can be found in AWS Cloudwatch. In general each process writes three logs: a standard out log, a standard error log and a usage log (the queries which have been run against the process remotely). Check these log files for errors.","title":"Troubleshooting"},{"location":"gettingstarted/#errors-in-cluster-creation","text":"On cluster creation, most errors will result in your cluster going to a \u201cCreate failed\u201d state. If that is the case you should: Click the cluster name in the \u201cCluster\u201d section of your environment Scroll down the page and open the \u201cLogs\u201d tab. This should have a message with a more individualised error you can check. If you click the LogStream for an individual log it will take you to AWS CloudWatch where you can filter the messages for keywords or for messages in a certain time window. It is worthwhile checking the logs even for clusters that have been created and searching for terms like \u201cerr\u201d, \u201cerror\u201d or \u201cfail\u201d","title":"Errors in cluster creation"},{"location":"gettingstarted/#make-it-your-own","text":"To customize it for a specific data set update the schema file (code/rdb/schema.q) and replace the feed process with a feed of data from a live system.","title":"Make It Your Own"},{"location":"terraformdeloyment/","text":"Terraform Deployment for TorQ in Finspace bundle and FinSpace Environment This Terraform setup is designed to deploy and manage a FinSpace environment running a TorQ in Finspace bundle. Prerequisites Ensure that you have the latest version of the AWS CLI installed. (Refer to Resource Link Section) Have the latest version of Terraform installed. (Refer to Resource Link Section) Configure the AWS CLI to your AWS account. (Refer to Resource Link Section) Create a KMS key in the region where you intend to set up your environment. You will also need to edit the key policy to grant FinSpace permissions. Note that FinSpace environments are limited to one per region. Make sure you don't already have an environment set up in the same region. Download this repository along with the latest version of TorQ. This instruction refers to Linux and would only work under the Linux environment. Resource Link For installing AWS CLI AWS Command Line Interface For Connecting AWS CLI to Your AWS Account AWS Sign-In For installing Terraform Install Terraform For detailed Terraform deployment instructions, refer to TorQ in Finspace Deployment / Terraform . How to Use - Initial Deployment (New User Please Follow This Section) (Optional) If you have an HDB you want to migrate to FinSpace, replace the dummy HDB in TorQ-Amazon-FinSpace-Starter-Pack/finTorq-App/hdb . Move into the TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments directory; this will be the Terraform working directory from which you should run all terraform commands. Modify variables inside the terraform.tfvars file, such as region name, environment name, database name. You can modify it by replacing the variable name inside of \"Name\" . For example, For the variable on role-name , you can change the variable name by replacing \"finspace-role\" . (Optional) If you have changed the database name from the default finspace-database to any other names, please also edit the env.q inside the finTorq-App directory, changing the database name to the new variable that you have set in line 19. Run aws configure in the terminal to set up your access key and secret key from your AWS account. This is needed to connect to your account and use the Terraform deployment. Check our resource link for more instructions on how to find your access key and secret key. From your Terraform working directory which is TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments , run terraform init . If initialized without error, run terraform plan . This will show all resources set to be created or destroyed by Terraform. Run terraform apply to execute this plan. The initial deployment can take approximately 45 minutes, and connection losses can cause errors with deployment, so it's a good idea to run this in nohup . (Using nohup might lead to a higher cost of operating the codes if you are using Terraform from a cloud environment.) Managing Your Infrastructure Once your environment is up and running, you can use this configuration to manage it: Code Updates: If you make any code changes in TorQ or TorQ-Amazon-FinSpace-Starter-Pack and want to apply these to your clusters, rezip these directories and run the Terraform deployment again. This will recreate clusters with the updated code. Cluster Config: If you want to make changes to a cluster's config settings (e.g., node size of the RDB), update this in clusters/rdb.tf and run Terraform again. The RDB will be recreated with the new node size. Delete/Create Clusters: Clusters can be deleted or created individually or all at once from the terraform.tfvars file. To delete a cluster, set its count to 0. To delete all clusters, set create-clusters to 0. Basic Commands in Terraform terraform init - Prepare your working directory for other commands terraform validate - Check whether the configuration is valid terraform plan - Show changes required by the current configuration terraform apply - Create or update infrastructure terraform destroy - Destroy previously-created infrastructure For more commands in Terraform, please visit Terraform Command Terraform State Management Terraform maintains a state file that tracks the state of the deployed infrastructure. This state file is crucial for Terraform to understand what resources have been created and to make changes to them. To ensure proper state management: Always store your state files securely, as they may contain sensitive information. Consider using remote state storage, such as Amazon S3, to keep your state files safe and accessible from multiple locations. Avoid manual changes to resources managed by Terraform, as this can lead to inconsistencies between the actual infrastructure and Terraform's state. Deploying With Terraform (User With Existing Infrastructure) For users with existing infrastructure in their AWS account who would like to reuse the same resources for their TorQ in Finspace bundle, you can use import blocks in Terraform. This functionality allows you to import existing infrastructure resources into Terraform, bringing them under Terraform's management. The import block records that Terraform imported the resource and did not create it. After importing, you can optionally remove import blocks from your configuration or leave them as a record of the resource's origin. Once imported, Terraform tracks the resource in your state file. You can then manage the imported resource like any other, updating its attributes and destroying it as part of a standard resource lifecycle. Move into the deployments directory, and you'll see an imports.tf file (currently empty). This imports.tf file is automatically run before Terraform applies any changes to the structure, importing existing structures from your AWS to the deployment system. Terraform Import Block Syntax import { to = aws_instance.example id = \"i-abcd1234\" } resource \"aws_instance\" \"example\" { name = \"hashi\" # (other resource arguments...) } The above import block defines an import of the AWS instance with the ID \"i-abcd1234\" into the aws_instance.example resource in the root module. The import block has the following arguments: to - The instance address this resource will have in your state file. id - A string with the import ID of the resource. `provider`` (optional) - An optional custom resource provider, see The Resource provider Meta-Argument for details. If you do not set the provider argument, Terraform attempts to import from the default provider. The import block's id argument can be a literal string of your resource's import ID, or an expression that evaluates to a string. Terraform needs this import ID to locate the resource you want to import. The import ID must be known at plan time for planning to succeed. If the value of id is only known after apply, terraform plan will fail with an error. The identifier you use for a resource's import ID is resource-specific. You can find the required ID in the provider documentation for the resource you wish to import. Terraform import block Template We have created a Terraform import block template in terraform-deployment/importtemplate.md . In this template, you can select the needed import block and paste it into the imports.tf file within the terraform-deployment/deployments/imports.tf directory. Remember to change the ID to the referring ID of your existing infrastructure. List of AWS Structures that will be created with our Terraform deployment module.environment.data.aws_iam_policy_document.iam-policy module.environment.data.aws_iam_policy_document.s3-code-policy module.environment.data.aws_iam_policy_document.s3-data-policy module.environment.aws_ec2_transit_gateway.test module.environment.aws_finspace_kx_database.database module.environment.aws_finspace_kx_environment.environment module.environment.aws_finspace_kx_user.finspace-user module.environment.aws_iam_policy.finspace-policy module.environment.aws_iam_role.finspace-test-role module.environment.aws_iam_role_policy_attachment.policy_attachment module.environment.aws_s3_bucket.finspace-code-bucket module.environment.aws_s3_bucket.finspace-data-bucket module.environment.aws_s3_bucket_policy.code-policy module.environment.aws_s3_bucket_policy.data-policy module.environment.aws_s3_bucket_public_access_block.code_bucket module.environment.aws_s3_bucket_public_access_block.data_bucket module.environment.aws_s3_bucket_versioning.versioning module.environment.null_resource.create_changeset module.environment.null_resource.upload_hdb module.lambda.data.aws_iam_policy_document.finspace-extra module.lambda.aws_cloudwatch_event_rule.trigger_finSpace-rdb-lambda module.lambda.aws_cloudwatch_event_target.target_finSpace-rdb-lambda module.lambda.aws_cloudwatch_metric_alarm.RDBOverCPUUtilization module.lambda.aws_iam_policy.lambda_basic_policy module.lambda.aws_iam_policy.lambda_ec2_policy module.lambda.aws_iam_policy.lambda_finspace_policy module.lambda.aws_iam_role.lambda_execution_role module.lambda.aws_iam_role_policy_attachment.attach1 module.lambda.aws_iam_role_policy_attachment.attach2 module.lambda.aws_iam_role_policy_attachment.attach3 module.lambda.aws_lambda_function.finSpace-rdb-lambda module.lambda.aws_lambda_permission.lambda_from_cw_permission module.network.aws_internet_gateway.finspace-igw module.network.aws_route.finspace-route module.network.aws_route_table.finspace-route-table module.network.aws_security_group.finspace-security-group module.network.aws_subnet.finspace-subnets[0] module.network.aws_subnet.finspace-subnets[1] module.network.aws_subnet.finspace-subnets[2] module.network.aws_subnet.finspace-subnets[3] module.network.aws_vpc.finspace-vpc References and Documentation For more in-depth information and documentation, explore the following resources: Terraform Documentation AWS Documentation These resources provide detailed information about Terraform and AWS services, best practices, and advanced configurations.","title":"Deploy Using Terraform"},{"location":"terraformdeloyment/#terraform-deployment-for-torq-in-finspace-bundle-and-finspace-environment","text":"This Terraform setup is designed to deploy and manage a FinSpace environment running a TorQ in Finspace bundle.","title":"Terraform Deployment for TorQ in Finspace bundle and FinSpace Environment"},{"location":"terraformdeloyment/#prerequisites","text":"Ensure that you have the latest version of the AWS CLI installed. (Refer to Resource Link Section) Have the latest version of Terraform installed. (Refer to Resource Link Section) Configure the AWS CLI to your AWS account. (Refer to Resource Link Section) Create a KMS key in the region where you intend to set up your environment. You will also need to edit the key policy to grant FinSpace permissions. Note that FinSpace environments are limited to one per region. Make sure you don't already have an environment set up in the same region. Download this repository along with the latest version of TorQ. This instruction refers to Linux and would only work under the Linux environment.","title":"Prerequisites"},{"location":"terraformdeloyment/#resource-link","text":"For installing AWS CLI AWS Command Line Interface For Connecting AWS CLI to Your AWS Account AWS Sign-In For installing Terraform Install Terraform For detailed Terraform deployment instructions, refer to TorQ in Finspace Deployment / Terraform .","title":"Resource Link"},{"location":"terraformdeloyment/#how-to-use-initial-deployment-new-user-please-follow-this-section","text":"(Optional) If you have an HDB you want to migrate to FinSpace, replace the dummy HDB in TorQ-Amazon-FinSpace-Starter-Pack/finTorq-App/hdb . Move into the TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments directory; this will be the Terraform working directory from which you should run all terraform commands. Modify variables inside the terraform.tfvars file, such as region name, environment name, database name. You can modify it by replacing the variable name inside of \"Name\" . For example, For the variable on role-name , you can change the variable name by replacing \"finspace-role\" . (Optional) If you have changed the database name from the default finspace-database to any other names, please also edit the env.q inside the finTorq-App directory, changing the database name to the new variable that you have set in line 19. Run aws configure in the terminal to set up your access key and secret key from your AWS account. This is needed to connect to your account and use the Terraform deployment. Check our resource link for more instructions on how to find your access key and secret key. From your Terraform working directory which is TorQ-Amazon-FinSpace-Starter-Pack/terraform-deployment/deployments , run terraform init . If initialized without error, run terraform plan . This will show all resources set to be created or destroyed by Terraform. Run terraform apply to execute this plan. The initial deployment can take approximately 45 minutes, and connection losses can cause errors with deployment, so it's a good idea to run this in nohup . (Using nohup might lead to a higher cost of operating the codes if you are using Terraform from a cloud environment.)","title":"How to Use - Initial Deployment (New User Please Follow This Section)"},{"location":"terraformdeloyment/#managing-your-infrastructure","text":"Once your environment is up and running, you can use this configuration to manage it: Code Updates: If you make any code changes in TorQ or TorQ-Amazon-FinSpace-Starter-Pack and want to apply these to your clusters, rezip these directories and run the Terraform deployment again. This will recreate clusters with the updated code. Cluster Config: If you want to make changes to a cluster's config settings (e.g., node size of the RDB), update this in clusters/rdb.tf and run Terraform again. The RDB will be recreated with the new node size. Delete/Create Clusters: Clusters can be deleted or created individually or all at once from the terraform.tfvars file. To delete a cluster, set its count to 0. To delete all clusters, set create-clusters to 0.","title":"Managing Your Infrastructure"},{"location":"terraformdeloyment/#basic-commands-in-terraform","text":"terraform init - Prepare your working directory for other commands terraform validate - Check whether the configuration is valid terraform plan - Show changes required by the current configuration terraform apply - Create or update infrastructure terraform destroy - Destroy previously-created infrastructure For more commands in Terraform, please visit Terraform Command","title":"Basic Commands in Terraform"},{"location":"terraformdeloyment/#terraform-state-management","text":"Terraform maintains a state file that tracks the state of the deployed infrastructure. This state file is crucial for Terraform to understand what resources have been created and to make changes to them. To ensure proper state management: Always store your state files securely, as they may contain sensitive information. Consider using remote state storage, such as Amazon S3, to keep your state files safe and accessible from multiple locations. Avoid manual changes to resources managed by Terraform, as this can lead to inconsistencies between the actual infrastructure and Terraform's state.","title":"Terraform State Management"},{"location":"terraformdeloyment/#deploying-with-terraform-user-with-existing-infrastructure","text":"For users with existing infrastructure in their AWS account who would like to reuse the same resources for their TorQ in Finspace bundle, you can use import blocks in Terraform. This functionality allows you to import existing infrastructure resources into Terraform, bringing them under Terraform's management. The import block records that Terraform imported the resource and did not create it. After importing, you can optionally remove import blocks from your configuration or leave them as a record of the resource's origin. Once imported, Terraform tracks the resource in your state file. You can then manage the imported resource like any other, updating its attributes and destroying it as part of a standard resource lifecycle. Move into the deployments directory, and you'll see an imports.tf file (currently empty). This imports.tf file is automatically run before Terraform applies any changes to the structure, importing existing structures from your AWS to the deployment system.","title":"Deploying With Terraform (User With Existing Infrastructure)"},{"location":"terraformdeloyment/#terraform-import-block-syntax","text":"import { to = aws_instance.example id = \"i-abcd1234\" } resource \"aws_instance\" \"example\" { name = \"hashi\" # (other resource arguments...) } The above import block defines an import of the AWS instance with the ID \"i-abcd1234\" into the aws_instance.example resource in the root module. The import block has the following arguments: to - The instance address this resource will have in your state file. id - A string with the import ID of the resource. `provider`` (optional) - An optional custom resource provider, see The Resource provider Meta-Argument for details. If you do not set the provider argument, Terraform attempts to import from the default provider. The import block's id argument can be a literal string of your resource's import ID, or an expression that evaluates to a string. Terraform needs this import ID to locate the resource you want to import. The import ID must be known at plan time for planning to succeed. If the value of id is only known after apply, terraform plan will fail with an error. The identifier you use for a resource's import ID is resource-specific. You can find the required ID in the provider documentation for the resource you wish to import.","title":"Terraform Import Block Syntax"},{"location":"terraformdeloyment/#terraform-import-block-template","text":"We have created a Terraform import block template in terraform-deployment/importtemplate.md . In this template, you can select the needed import block and paste it into the imports.tf file within the terraform-deployment/deployments/imports.tf directory. Remember to change the ID to the referring ID of your existing infrastructure.","title":"Terraform import block Template"},{"location":"terraformdeloyment/#list-of-aws-structures-that-will-be-created-with-our-terraform-deployment","text":"module.environment.data.aws_iam_policy_document.iam-policy module.environment.data.aws_iam_policy_document.s3-code-policy module.environment.data.aws_iam_policy_document.s3-data-policy module.environment.aws_ec2_transit_gateway.test module.environment.aws_finspace_kx_database.database module.environment.aws_finspace_kx_environment.environment module.environment.aws_finspace_kx_user.finspace-user module.environment.aws_iam_policy.finspace-policy module.environment.aws_iam_role.finspace-test-role module.environment.aws_iam_role_policy_attachment.policy_attachment module.environment.aws_s3_bucket.finspace-code-bucket module.environment.aws_s3_bucket.finspace-data-bucket module.environment.aws_s3_bucket_policy.code-policy module.environment.aws_s3_bucket_policy.data-policy module.environment.aws_s3_bucket_public_access_block.code_bucket module.environment.aws_s3_bucket_public_access_block.data_bucket module.environment.aws_s3_bucket_versioning.versioning module.environment.null_resource.create_changeset module.environment.null_resource.upload_hdb module.lambda.data.aws_iam_policy_document.finspace-extra module.lambda.aws_cloudwatch_event_rule.trigger_finSpace-rdb-lambda module.lambda.aws_cloudwatch_event_target.target_finSpace-rdb-lambda module.lambda.aws_cloudwatch_metric_alarm.RDBOverCPUUtilization module.lambda.aws_iam_policy.lambda_basic_policy module.lambda.aws_iam_policy.lambda_ec2_policy module.lambda.aws_iam_policy.lambda_finspace_policy module.lambda.aws_iam_role.lambda_execution_role module.lambda.aws_iam_role_policy_attachment.attach1 module.lambda.aws_iam_role_policy_attachment.attach2 module.lambda.aws_iam_role_policy_attachment.attach3 module.lambda.aws_lambda_function.finSpace-rdb-lambda module.lambda.aws_lambda_permission.lambda_from_cw_permission module.network.aws_internet_gateway.finspace-igw module.network.aws_route.finspace-route module.network.aws_route_table.finspace-route-table module.network.aws_security_group.finspace-security-group module.network.aws_subnet.finspace-subnets[0] module.network.aws_subnet.finspace-subnets[1] module.network.aws_subnet.finspace-subnets[2] module.network.aws_subnet.finspace-subnets[3] module.network.aws_vpc.finspace-vpc","title":"List of AWS Structures that will be created with our Terraform deployment"},{"location":"terraformdeloyment/#references-and-documentation","text":"For more in-depth information and documentation, explore the following resources: Terraform Documentation AWS Documentation These resources provide detailed information about Terraform and AWS services, best practices, and advanced configurations.","title":"References and Documentation"}]}